{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbec6beb",
   "metadata": {},
   "source": [
    "M A S K I N G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce73a573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def kernel(xptr, yptr, outptr, nelements, BLOCKSIZE: tl.constexpr):\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    offsets = pid * BLOCKSIZE + tl.arange(0, BLOCKSIZE)\n",
    "    mask = offsets < nelements\n",
    "\n",
    "    x = tl.load(xptr + offsets, mask=mask, other=0.0)\n",
    "    y = tl.load(yptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    acc = x + y\n",
    "    acc = tl.sum(acc, axis=0)\n",
    "\n",
    "    tl.store(outptr + pid, acc)\n",
    "\n",
    "\n",
    "def test(x, y):\n",
    "    nelements = x.numel()\n",
    "    BLOCKSIZE = 1024\n",
    "\n",
    "    grid = lambda meta: (triton.cdiv(nelements, meta[\"BLOCKSIZE\"]),)\n",
    "\n",
    "    out = torch.empty((grid({\"BLOCKSIZE\": BLOCKSIZE})[0],), device=\"cuda\")\n",
    "\n",
    "    kernel[grid](x, y, out, nelements, BLOCKSIZE)\n",
    "\n",
    "    return out.sum()\n",
    "\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], device=\"cuda\")\n",
    "y = torch.tensor([2.0, 3.0, 4.0, 5.0], device=\"cuda\")\n",
    "\n",
    "res = test(x, y)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3c9914f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25., 49., 97.], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch , triton , math\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def kernel(xptr , outptr , threshold , nelements , blocksize : tl.constexpr):\n",
    "    pid = tl.program_id(axis = 0)\n",
    "\n",
    "    offsets = pid * blocksize + tl.arange(0 , blocksize)\n",
    "    mask = offsets < nelements\n",
    "\n",
    "    x = tl.load(xptr + offsets , mask = mask , other = 0.0)\n",
    "    sqmask = (x > threshold)\n",
    "\n",
    "    acc = tl.where(sqmask , x * x , 0)\n",
    "    acc = tl.sum(acc , axis = 0)\n",
    "\n",
    "    tl.store(outptr + pid , acc)\n",
    "\n",
    "def test(x):\n",
    "    nelements = x.numel()\n",
    "    blocksize = 2\n",
    "\n",
    "    out = torch.zeros(math.ceil(nelements / blocksize) , device = 'cuda')\n",
    "    grid = grid = lambda meta: (triton.cdiv(nelements , meta['blocksize']), )\n",
    "\n",
    "    kernel[grid](x , out , 3 , nelements , blocksize)\n",
    "\n",
    "    return out\n",
    "\n",
    "x = torch.tensor([1.0 , 5.0 , 2.0 , 7.0 , 4.0 , 9.0], device='cuda')\n",
    "res = test(x)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f2632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 7., 5., 9.], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@triton.jit\n",
    "def kernel(xptr, outptr, nelements, threshold, counter, blocksize: tl.constexpr):\n",
    "    pid = tl.program_id(0)\n",
    "    offsets = pid * blocksize + tl.arange(0, blocksize)\n",
    "    mask = offsets < nelements\n",
    "\n",
    "    x = tl.load(xptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    \n",
    "    x_mask = x > threshold\n",
    "    combined_mask = mask & x_mask\n",
    "\n",
    "    valid_ind = tl.where(combined_mask , 1 , 0)\n",
    "    block_count = tl.sum(valid_ind, axis=0)\n",
    "    start_idx = tl.atomic_add(counter, block_count)\n",
    "\n",
    "    \n",
    "    prefix = tl.cumsum(valid_ind, axis=0) - valid_ind\n",
    "    out_pos = start_idx + prefix\n",
    "\n",
    "    tl.store(outptr + out_pos, x, mask=combined_mask)\n",
    "\n",
    "def test(x: torch.Tensor, threshold: float):\n",
    "    x = x.contiguous()\n",
    "    out = torch.empty_like(x)\n",
    "    out_counter = torch.zeros(1, dtype=torch.int32, device=x.device)\n",
    "\n",
    "    BLOCK_SIZE = 128\n",
    "    grid = (triton.cdiv(x.numel(), BLOCK_SIZE),)\n",
    "\n",
    "    kernel[grid](\n",
    "        x,\n",
    "        out,\n",
    "        x.numel(),\n",
    "        threshold,\n",
    "        out_counter,\n",
    "        blocksize=BLOCK_SIZE   \n",
    "    )\n",
    "\n",
    "    return out[:out_counter.item()]\n",
    "\n",
    "x = torch.tensor([1.0 , 5.0 , 2.0 , 7.0 , 5.0 , 9.0], device='cuda')\n",
    "res = test(x , 4)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903d325",
   "metadata": {},
   "source": [
    "S T R I D E S "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ec938eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1696, -0.5476,  1.2832,  ...,  1.4566, -0.6243, -0.3408],\n",
       "        [-0.6959,  0.7655, -0.8117,  ...,  1.3591, -1.0552, -1.1866],\n",
       "        [-1.0471,  0.9832,  1.0137,  ..., -0.4676, -0.2475,  1.6505],\n",
       "        ...,\n",
       "        [-1.6181, -1.0190,  0.0295,  ...,  0.0285, -0.2558,  0.0209],\n",
       "        [-0.0846, -0.6835, -0.2950,  ...,  0.6142,  0.3847, -0.7668],\n",
       "        [-0.5564,  1.6412,  1.6846,  ..., -1.9642,  0.7100,  1.4516]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch , triton \n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def transposekernel(xptr , outptr , xstrm , xstrn , outstrm , outstrn , M , N , MBLOCK: tl.constexpr , NBLOCK: tl.constexpr):\n",
    "    pid = tl.program_id(axis = 0)\n",
    "    ypid = tl.program_id(axis = 1)\n",
    "\n",
    "    row = pid * MBLOCK + tl.arange(0 , MBLOCK)\n",
    "    col = ypid * NBLOCK + tl.arange(0 , NBLOCK)\n",
    "\n",
    "    ptrx = xptr + row[: , None] * xstrm + col[None , :] * xstrn\n",
    "    mask = (row[: , None] < M) & (col[None , :] < N)\n",
    "\n",
    "    x = tl.load(ptrx , mask = mask , other = 0)\n",
    "    out = outptr + col[: , None] * outstrm + row[None , :] * outstrn\n",
    "    maskout = (col[None , :] < N) & (row[: , None] < M)\n",
    "\n",
    "    x_transposed = tl.trans(x)\n",
    "    tl.store(out , x_transposed , mask = maskout)\n",
    "\n",
    "def test(x):\n",
    "    M , N = x.shape\n",
    "    mblock , nblock = 8 , 8\n",
    "\n",
    "    out = torch.empty((N , M) , device = 'cuda')\n",
    "    grid = (\n",
    "        (M + mblock - 1) // mblock,\n",
    "        (N + nblock - 1) // nblock,\n",
    "    )\n",
    "\n",
    "    transposekernel[grid](x , out , x.stride()[0] , x.stride()[1] , out.stride()[0] , out.stride()[1] , M , N , mblock , nblock)\n",
    "    return out\n",
    "\n",
    "x = torch.randn(128 , 64 , device = 'cuda')\n",
    "\n",
    "res = test(x)\n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def indexkernel(xptr , outptr , index , xstrx , xstry):\n",
    "\n",
    "    x , y = index\n",
    "\n",
    "    ptr = xptr + x[: , None] * xstrx + y[None , :] * xstry\n",
    "\n",
    "    tl.store(outptr , tl.load(ptr))\n",
    "\n",
    "def test(x , index):\n",
    "    out = torch.empty((1,) , device = 'cuda')\n",
    "    indexkernel[(1,)](x , out , index , x.stride()[0] , x.stride()[1])\n",
    "    return out\n",
    "\n",
    "x = torch.tensor([[1.0 , 2.0 , 3.0]] , device = 'cuda')\n",
    "index = (0 , 1)\n",
    "\n",
    "res = test(x , index)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e24131",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12).reshape(3,4) # 4 , 1\n",
    "a = x.t()# 1 , 4\n",
    "b = x[:, ::2] # 4 , 2\n",
    "c = x[::2, :] # 8 , 1\n",
    "d = x.permute(1,0) # 1 , 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "65dc0f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).reshape(4 , 4)\n",
    "y = x.t()\n",
    "y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0beb806c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  4,  8, 12],\n",
       "         [ 1,  5,  9, 13],\n",
       "         [ 2,  6, 10, 14],\n",
       "         [ 3,  7, 11, 15]]),\n",
       " tensor([[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most likely the reason is as memory was never changed just the layout , but view depends on layout , but the actual tensor size didn't matched \n",
    "y , x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "715b6ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca864782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  4,  8, 12,  1,  5,  9, 13,  2,  6, 10, 14,  3,  7, 11, 15])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38a47508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  4,  8, 12,  1,  5,  9, 13,  2,  6, 10, 14,  3,  7, 11, 15])\n"
     ]
    }
   ],
   "source": [
    "y_flat = y.contiguous().view(-1)\n",
    "print(y_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9359ac44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]]),\n",
       " tensor([[ 0,  4,  8],\n",
       "         [ 1,  5,  9],\n",
       "         [ 2,  6, 10],\n",
       "         [ 3,  7, 11]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3,4) \n",
    "transposed = torch.as_strided(x , (4 , 3) , (1 , 4))\n",
    "x.t() , transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "53b17ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 5],\n",
       "        [10],\n",
       "        [15]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).reshape(4,4) # 4 , 1\n",
    "diagnol = torch.as_strided(x , (4 , 1) , (5 , 1))\n",
    "diagnol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dd2469d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2547, 1.0185, 0.9231, 1.3405, 0.8570, 1.1698], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@triton.jit\n",
    "def flattenkernel(\n",
    "    xptr, outptr, xstrm, xstrn, M, N, Mblock: tl.constexpr, Nblock: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(0)\n",
    "    ypid = tl.program_id(1)\n",
    "\n",
    "    row = pid * Mblock + tl.arange(0, Mblock)\n",
    "    col = ypid * Nblock + tl.arange(0, Nblock)\n",
    "\n",
    "    ptr = xptr + row[:, None] * xstrm + col[None, :] * xstrn\n",
    "    mask = (row[:, None] < M) & (col[None, :] < N)\n",
    "\n",
    "    out = outptr + row[:, None] * N + col[None, :]\n",
    "\n",
    "    vals = tl.load(ptr, mask=mask)\n",
    "\n",
    "    tl.store(out, vals, mask=mask)\n",
    "\n",
    "def test(x):\n",
    "    m , n = x.shape\n",
    "    out = torch.empty((m * n) , device = 'cuda')\n",
    "    mblock , nblock = 32 , 32\n",
    "\n",
    "    grid = (\n",
    "        (m + mblock - 1) // mblock,\n",
    "        (n + nblock - 1) // nblock,\n",
    "    )\n",
    "\n",
    "    flattenkernel[grid](x , out , x.stride()[0] , x.stride()[1] , m , n , mblock , nblock)\n",
    "    return out\n",
    "\n",
    "x = torch.randn(2 , 3 , device = 'cuda')\n",
    "res = test(x)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b7335db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [1., 4.]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2.0 , 3.0] , [1.0 , 4.0]] , device = 'cuda')\n",
    "y = torch.as_strided(x , (2 , 2) , (2 , 1) )\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa085ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 4.],\n",
       "         [2., 5.],\n",
       "         [3., 6.]], device='cuda:0'),\n",
       " None,\n",
       " tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.]], device='cuda:0'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([[1.0 , 2.0 , 3.0] , [4.0 , 5.0 , 6.0]] , device = 'cuda')\n",
    "x.permute(1 , 0) , print(\" \\n\"), x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "440abe0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 4.],\n",
       "        [2., 5.],\n",
       "        [3., 6.]], device='cuda:0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permute = torch.as_strided(x , (3 , 2) , (1 , 3))\n",
    "permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e8a47a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 3],\n",
       "        [2, 4]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "y = x.as_strided((3,2),(1,2))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df89c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(16).reshape(4,4)\n",
    "y = x[:, ::2].contiguous()\n",
    "y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557d048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton_env (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
