{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def casual_mask(seq , pad_idx = 0):\n",
    "    mask = (seq == pad_idx).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    #or\n",
    "\n",
    "    mask = torch.tril((seq.size(1) , seq.size(1)))\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8006558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ffn\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self , hid_dim , embed , dropout : float):\n",
    "        super().__init__()        # we project in high dims\n",
    "        #we will use manual instead of linear projection , we will be using transpose to match architecture of eco-system\n",
    "        self.ln1 = nn.Parameter(torch.randn(hid_dim , embed)) #(hid_dim , embed)\n",
    "        self.ln1B = nn.Parameter(torch.randn(hid_dim))\n",
    "\n",
    "        self.ln2 = nn.Parameter(torch.randn(embed , hid_dim))\n",
    "        self.ln2B = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ln1)\n",
    "        nn.init.zeros_(self.ln1B)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ln2)\n",
    "        nn.init.zeros_(self.ln2B)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self , x):\n",
    "        # x.shape = [batch , seq , embed]\n",
    "        hidden = self.act(x @ self.ln1.T + self.ln1B)# [batch , seq , hid]\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = hidden @ self.ln2.T + self.ln2B\n",
    "\n",
    "        return output\n",
    "    \n",
    "class LayerNormalizaton(nn.Module):\n",
    "    def __init__(self , features , eps = 1e-4):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(features))\n",
    "        self.bias = nn.Parameter(torch.randn(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        std = x.std(dim = -1 , keepdim = True)\n",
    "\n",
    "        return (x - mean) / (std + self.eps) * self.alpha + self.bias\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self , features , dropout : float):\n",
    "        super().__init__()\n",
    "        self.layer1 = LayerNormalizaton(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer1(x)))\n",
    "    \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self , embed , vocab):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab , embed)\n",
    "        self.embed_dim = embed\n",
    "    \n",
    "    def forward(self , x):\n",
    "        output = self.embed(x) * math.sqrt(self.embed_dim)\n",
    "        return output\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self , embed , seq_len , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pos = torch.arange(0 , seq_len , dtype = torch.float32).unsqueeze(1)\n",
    "        term = torch.exp(torch.arange(0 , embed , 2).float() * -math.log(10000) / embed)\n",
    "        pe = torch.zeros(seq_len , embed)\n",
    "\n",
    "        pe[: , 0::2] = torch.sin(pos * term)\n",
    "        pe[: , 1::2] = torch.cos(pos * term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe' , pe)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.dropout(x + (self.pe[: , :x.shape[1] , :].requires_grad_(False)))\n",
    "    \n",
    "class MulticlassAttention(nn.Module):\n",
    "    def __init__(self , embed , num_head , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.num_head = num_head\n",
    "        assert (embed % num_head == 0) , \"dusra try kr\" \n",
    "\n",
    "        self.dk = embed // num_head\n",
    "\n",
    "        self.q = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.q_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.k = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.k_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.v = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.v_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.o = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.o_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        for name in [self.q , self.k , self.v , self.o]:\n",
    "            nn.init.xavier_uniform_(name)\n",
    "        for name in [self.q_bias , self.k_bias , self.v_bias , self.o_bias]:\n",
    "            nn.init.zeros_(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(q , k , v , mask , dropout , pastlayer):\n",
    "        dk = q.size(-1)\n",
    "\n",
    "        if pastlayer is not None:\n",
    "            k_ , v_ = pastlayer\n",
    "            k = torch.cat([k_ , k] , dim = -2)\n",
    "            v = torch.cat([v_ , v] , dim = -2)\n",
    "        \n",
    "        present = (k , v)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2 , -1)) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            \n",
    "            current_kv_len = k.size(-2)\n",
    "            current_q_len = q.size(-2)\n",
    "            \n",
    "            if pastlayer is not None:\n",
    "                causal_mask = torch.tril(torch.ones(current_kv_len, current_kv_len, device=scores.device))\n",
    "\n",
    "                causal_mask = causal_mask[-current_q_len:, :].unsqueeze(0).unsqueeze(0)\n",
    "                if mask.size(-1) >= current_kv_len:\n",
    "                    \n",
    "                    padding_mask = mask[..., -current_kv_len:]\n",
    "                else:\n",
    "                    padding_mask = torch.ones_like(mask[..., :current_kv_len])\n",
    "                causal_mask = causal_mask.expand(mask.size(0), -1, -1, -1)\n",
    "                \n",
    "                # Combine masks\n",
    "                mask = causal_mask * padding_mask\n",
    "                print(f\"  combined mask shape: {mask.shape}\")\n",
    "                \n",
    "            else:\n",
    "                if mask.size(-1) != current_kv_len:\n",
    "                    mask = mask[..., :current_kv_len]\n",
    "                \n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        max_ = torch.max(scores , dim = -1 , keepdim = True)[0]\n",
    "        sc = torch.exp(scores - max_)\n",
    "        scores = sc / torch.sum(sc , dim = -1 , keepdim = True)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        \n",
    "        return (scores @ v), present\n",
    "        \n",
    "\n",
    "    def forward(self , q , k , v , mask = None , pastlayer = None): #batch , seq , embed\n",
    "        q = q @ self.q + self.q_bias\n",
    "        k = k @ self.k + self.k_bias\n",
    "        v = v @ self.v + self.v_bias\n",
    "\n",
    "        batch = q.size(0)\n",
    "        seq = q.size(1)\n",
    "        k_seq = k.size(1)\n",
    "\n",
    "        query = q.view(batch , seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "        key = k.view(batch , k_seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "        value = v.view(batch , k_seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "\n",
    "        attn , present = MulticlassAttention.attention(query , key , value , mask , self.dropout , pastlayer)\n",
    "\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous().view(batch , seq , self.embed)\n",
    "\n",
    "        return attn @ self.o + self.o_bias , present\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , attn , ffn , feat , dropout : float):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.res1 = Residual(feat , dropout)\n",
    "        self.res2 = Residual(feat , dropout)\n",
    "\n",
    "    def forward(self , x , mask):\n",
    "        selfattn , _ = self.attn(x , x , x , mask , pastlayer = None)\n",
    "        x = self.res1(x , lambda _: selfattn)\n",
    "        x = self.res2(x , lambda a: self.ffn(a))\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self , feat , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalizaton(feat)\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self , x , mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x , mask)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self , selfattn , crossattn , ffn , feat , dropout : float):\n",
    "        super().__init__()\n",
    "        self.selfattn = selfattn\n",
    "        self.crossattn = crossattn\n",
    "        self.ffn = ffn\n",
    "        self.res1 = Residual(feat , dropout)\n",
    "        self.res2 = Residual(feat , dropout)\n",
    "        self.res3 = Residual(feat , dropout)\n",
    "    \n",
    "    def forward(self , x , encoderout , selfmask , crossmask , pastlayer):\n",
    "        attnpresent , _ = pastlayer if pastlayer else (None , None)\n",
    "\n",
    "        selfattn , selfpresent = self.selfattn(x , x , x , selfmask , attnpresent)\n",
    "        crossattn , _ = self.crossattn(x , encoderout , encoderout , crossmask , pastlayer = None)\n",
    "\n",
    "        x = self.res1(x , lambda _ : selfattn)\n",
    "        x = self.res2(x , lambda _ : crossattn)\n",
    "        x = self.res3(x , lambda x : self.ffn(x))\n",
    "\n",
    "        return x , (selfpresent , None)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , feat , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalizaton(feat)\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self , x , enc , tgt_mask , src_mask , pastvalues):\n",
    "        new = []\n",
    "\n",
    "        for i , layer in enumerate(self.layers):\n",
    "            past = pastvalues[i] if pastvalues else None\n",
    "            x , layerpast = layer(x , enc , tgt_mask , src_mask , past)\n",
    "\n",
    "            new.append(layerpast)\n",
    "        \n",
    "        return self.norm(x) , new\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self , embed , vocab):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Parameter(torch.randn(vocab , embed))\n",
    "        self.bias = nn.Parameter(torch.randn(vocab))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self , x):\n",
    "        out = x @ self.linear.T + self.bias\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self , encoder : Encoder , decoder : Decoder , src_emb , tgt_emb , src_pos , tgt_pos , proj_layer):\n",
    "        super().__init__()\n",
    "        self.encoder_ = encoder\n",
    "        self.decoder_ = decoder\n",
    "        self.src_emb = src_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer \n",
    "    \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder_(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask, src_mask, pastvalues=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder_(tgt, encoder_out, tgt_mask, src_mask, pastvalues)\n",
    "    \n",
    "    def projection(self, x):\n",
    "        return self.proj_layer(x)\n",
    "\n",
    "def build(src_vocab , tgt_vocab , src_seq , tgt_seq , embed = 512 , num_head = 8 , num_layer = 6 , dropout =  float(0.1) , hid_dim = 2048):\n",
    "    src_embed = Embedding(embed = embed , vocab = src_vocab)\n",
    "    tgt_embed = Embedding(embed = embed , vocab = tgt_vocab)\n",
    "\n",
    "    src_pos = PositionalEmbedding(embed = embed , seq_len = src_seq , dropout = dropout)\n",
    "    tgt_pos = PositionalEmbedding(embed = embed , seq_len = tgt_seq , dropout = dropout)\n",
    "\n",
    "    enc_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        enc_attn = MulticlassAttention(embed , num_head , dropout )\n",
    "        ffn = FFN(hid_dim , embed , dropout)\n",
    "        enc_block = EncoderBlock(attn = enc_attn , ffn = ffn , feat = embed , dropout = dropout)\n",
    "        enc_blocks.append(enc_block)\n",
    "\n",
    "    dec_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        dec_selfAttn = MulticlassAttention(embed , num_head , dropout)\n",
    "        dec_crossAttn = MulticlassAttention(embed , num_head , dropout)\n",
    "        ffn = FFN(hid_dim , embed , dropout)\n",
    "        dec_block = DecoderBlock(selfattn = dec_selfAttn , crossattn = dec_crossAttn , ffn = ffn , feat = embed , dropout = dropout)\n",
    "        dec_blocks.append(dec_block)\n",
    "\n",
    "    encoder = Encoder(feat = embed , layers = nn.ModuleList(enc_blocks))\n",
    "    decoder = Decoder(feat = embed , layers = nn.ModuleList(dec_blocks))\n",
    "\n",
    "    proj_layer = Projection(embed = embed , vocab = tgt_vocab)\n",
    "\n",
    "    transformer = Transformer(encoder = encoder , decoder = decoder , src_emb = src_embed , tgt_emb = tgt_embed , src_pos = src_pos , tgt_pos = tgt_pos , proj_layer = proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "        \n",
    "transformer = build(src_vocab = 50 , tgt_vocab = 50 , src_seq = 8 , tgt_seq = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a6072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self , embed_dim , hid_dim , dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Parameter(torch.randn(hid_dim , embed_dim)) #(hid , embed)\n",
    "        self.l1bias = nn.Parameter(torch.randn(hid_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear2 = nn.Parameter(torch.randn(embed_dim , hid_dim)) #(embed , hid)\n",
    "        self.l2bias = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear1)\n",
    "        nn.init.xavier_uniform_(self.linear2)\n",
    "        nn.init.zeros_(self.l1bias)\n",
    "        nn.init.zeros_(self.l2bias)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self , x): #x -> (batch , seq , embed) \n",
    "        # batch , seq , hid\n",
    "        hidden = self.act(x @ self.linear1.T + self.l1bias) #(batch , seq , embed) @ (embed , hid) -> (batch , seq , hid)\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = hidden @ self.linear2.T + self.l2bias #(batch , seq , hid) @ \n",
    "        return  output \n",
    "    # we project to higher dims to have more features to be learned. \n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self , features , eps = 10 **-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        std  = x.std(dim = -1 , keepdim  = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self , features , dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # sublayer is layer we skip while doing connection\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size , embed_dim) # row , columns   \n",
    "\n",
    "    def forward(self , x):\n",
    "        out = self.embed(x) * math.sqrt(self.embed_dim)\n",
    "        return  out\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self , embed_dim , seq_len , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        position = torch.arange(0 , seq_len , dtype = torch.float).unsqueeze(1) \n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "\n",
    "        # (sin(position / 10000 ** (2i / embed-dim))) lets call 10000 ** (2i / embed-dim)) term\n",
    "        term = torch.exp(torch.arange(0 , embed_dim , 2).float() * -math.log(10000.0) / embed_dim)\n",
    "        pe[: , 0::2] = torch.sin(position * term)\n",
    "        pe[: , 1::2] = torch.cos(position * term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self , z):\n",
    "        # z -> (batch , seq_len , embed_dim)\n",
    "        return self.dropout(z + (self.pe[: , :z.shape[1] , :]).requires_grad_(False))\n",
    "\n",
    "def RoPE(x, sin, cos, start_pos=0):\n",
    "    seq = x.size(-2)\n",
    "    head_dim = x.size(-1)\n",
    "    num_head_x = x.size(1)  # This could be 8 (query) or 1 (key)\n",
    "    \n",
    "    embed_dim = sin.size(-1)\n",
    "    total_heads = embed_dim // head_dim  # This should be 8\n",
    "    \n",
    "    sin_sliced = sin[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    cos_sliced = cos[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    \n",
    "    sin_sliced = sin_sliced.view(seq, total_heads, head_dim)\n",
    "    cos_sliced = cos_sliced.view(seq, total_heads, head_dim)\n",
    "    \n",
    "    if total_heads != num_head_x:\n",
    "        sin_sliced = sin_sliced[:, :num_head_x, :]\n",
    "        cos_sliced = cos_sliced[:, :num_head_x, :]\n",
    "    \n",
    "    sin_sliced = sin_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    cos_sliced = cos_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    \n",
    "   \n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    \n",
    "    sin_even = sin_sliced[..., 0::2]\n",
    "    sin_odd = sin_sliced[..., 1::2]\n",
    "    cos_even = cos_sliced[..., 0::2]\n",
    "    cos_odd = cos_sliced[..., 1::2]\n",
    "    \n",
    "    rotated = torch.empty_like(x)\n",
    "    rotated[..., 0::2] = x_even * cos_even - x_odd * sin_even\n",
    "    rotated[..., 1::2] = x_even * sin_odd + x_odd * cos_odd\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "def RoPE_Embed(dim , seq):\n",
    "    \n",
    "    term = 1.0 / (10000 ** (torch.arange(0 , dim , 2).float() / dim))\n",
    "    seq_ = torch.arange(seq).float()\n",
    "\n",
    "    emb = torch.outer(seq_ , term)\n",
    "    emb = torch.cat([emb , emb] , dim = -1)\n",
    "\n",
    "    return emb.sin() , emb.cos()\n",
    "\n",
    "class MulticlassAttention(nn.Module):\n",
    "    def __init__(self , embed , num_head , dropout , max_seq_len = 5000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed\n",
    "        self.num_head = num_head\n",
    "\n",
    "        assert embed % num_head == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.dim = embed // num_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.wq = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.qb = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.wk = nn.Parameter(torch.randn(embed , self.dim))\n",
    "        self.kb = nn.Parameter(torch.randn(self.dim))\n",
    "\n",
    "        self.wv = nn.Parameter(torch.randn(embed , self.dim))\n",
    "        self.vb = nn.Parameter(torch.randn(self.dim))\n",
    "\n",
    "\n",
    "\n",
    "        # self.wq = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "        # self.wv = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "        # self.wk = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "\n",
    "        \n",
    "        self.wo = nn.Parameter(torch.randn(embed , embed))\n",
    "        # self.wo = nn.Linear(embed_dim , embed_dim , bias = False)\n",
    "\n",
    "        for param in [self.wq, self.wk, self.wv, self.wo]:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        for bias in [self.qb, self.kb, self.vb]:\n",
    "            nn.init.zeros_(bias)\n",
    "\n",
    "        self.register_buffer('sin', None)\n",
    "        self.register_buffer('cos', None)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _precompute_rope(self, seq_len):\n",
    "        if self.sin is None or self.sin.size(0) < seq_len:\n",
    "            sin, cos = RoPE_Embed(self.embed_dim, seq_len)\n",
    "            self.register_buffer('sin', sin)\n",
    "            self.register_buffer('cos', cos)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout, pastlayer=None):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        if pastlayer is not None:\n",
    "            past_k, past_v = pastlayer\n",
    "            # Past first, then current\n",
    "            key = torch.cat([past_k, key], dim=-2)\n",
    "            value = torch.cat([past_v, value], dim=-2)\n",
    "\n",
    "        present = (key, value)\n",
    "\n",
    "        # Compute attention scores\n",
    "        #(batch , num_head , seq , d_k(dim)) @ (batch , 1 , d_k(dim) , seq) \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        print(f\"  scores shape: {scores.shape}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            print(f\"  original mask shape: {mask.shape}\")\n",
    "            \n",
    "            current_kv_len = key.size(-2)\n",
    "            current_q_len = query.size(-2)\n",
    "            \n",
    "            # CRITICAL FIX: Create proper mask for current sequence length\n",
    "            if pastlayer is not None:\n",
    "                # Create causal mask for the current total sequence length\n",
    "                causal_mask = torch.tril(torch.ones(current_kv_len, current_kv_len, device=scores.device))\n",
    "\n",
    "                causal_mask = causal_mask[-current_q_len:, :].unsqueeze(0).unsqueeze(0)\n",
    "                if mask.size(-1) >= current_kv_len:\n",
    "                    \n",
    "                    padding_mask = mask[..., -current_kv_len:]\n",
    "                else:\n",
    "                    padding_mask = torch.ones_like(mask[..., :current_kv_len])\n",
    "                causal_mask = causal_mask.expand(mask.size(0), -1, -1, -1)\n",
    "                \n",
    "                # Combine masks\n",
    "                mask = causal_mask * padding_mask\n",
    "                print(f\"  combined mask shape: {mask.shape}\")\n",
    "                \n",
    "            else:\n",
    "                if mask.size(-1) != current_kv_len:\n",
    "                    mask = mask[..., :current_kv_len]\n",
    "                \n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        max_ = torch.max(scores , dim = -1 , keepdim = True)[0]\n",
    "        sc = torch.exp(scores - max_)\n",
    "        scores = sc / torch.sum(sc , dim = -1 , keepdim = True)\n",
    "        # scores = scores.softmax(dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        return (scores @ value), present\n",
    "    \n",
    "    def forward(self , q , k , v , mask = None , pastlayer = None , start_pos = 0):\n",
    "        batch = q.size(0)\n",
    "        \n",
    "\n",
    "        query = q @ self.wq + self.qb # (batch , seq_len , embed_dim)\n",
    "        key = k @ self.wk + self.kb\n",
    "        value = v @ self.wv + self.vb\n",
    "\n",
    "        \n",
    "        # (batch , seq_len , d_model(embed_dim)) -> (batch , seq , num_head , d_k(dim))\n",
    "        # transpose (batch , seq , num_head , d_k(dim)) -> (batch , num_head , seq , d_k(dim))\n",
    "        # by transposing it can work parallelly like take a batch , take head then give the seq (words)\n",
    "        query = query.reshape(batch , q.size(1) , self.num_head , self.dim).permute(0 , 2 , 1 , 3)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "\n",
    "        # key = key.reshape(batch , k.size(1) , 1 , self.dim).permute(0 , 2 , 1 , 3)\n",
    "        # value = value.reshape(batch , v.size(1) , 1 , self.dim).permute(0 , 2 , 1 , 3)\n",
    "\n",
    "        total_seq = max(q.size(1) + start_pos, \n",
    "                       k.size(1) + (pastlayer[0].size(2) if pastlayer else 0))\n",
    "        self._precompute_rope(total_seq)\n",
    "        query = RoPE(query, self.sin, self.cos, start_pos)\n",
    "        key = RoPE(key, self.sin, self.cos, 0)\n",
    "\n",
    "        out , pastlayer_ = MulticlassAttention.attention(query , key , value , mask , self.dropout , pastlayer)\n",
    "        # (batch , num_head , seq , d_k) -> # (batch , seq , num_head , d_k) -> (batch , seq , d_model)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch , q.size(1) , self.embed_dim)\n",
    "   \n",
    "        return out @ self.wo , pastlayer_# (batch , seq , d_model) \n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , attention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.ffn = ffn\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "\n",
    "        \n",
    "    def forward(self , x , mask):\n",
    "        \n",
    "        selfattn , _ = self.attention(x , x , x , mask)\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda x: self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.layer = layers\n",
    "\n",
    "    def forward(self , x , mask):\n",
    "        for layer in self.layer:\n",
    "            x = layer(x , mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self , selfAttention , crossAttention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = selfAttention\n",
    "        self.ffn = ffn\n",
    "        self.cross = crossAttention\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "        self.residual3 = Residual(feat , dropout)\n",
    "\n",
    "    def forward(self , x , enoderOutput , selfmask , crossmask , pastlayergroup):\n",
    "        selfpast , _ = pastlayergroup if pastlayergroup is not None else (None , None)\n",
    "        \n",
    "        selfattn , selfpresent = self.attention(x , x , x , selfmask , pastlayer = selfpast)\n",
    "        crossattn , crosspresent = self.cross(x , enoderOutput , enoderOutput , crossmask , pastlayer = None)\n",
    "\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda _: crossattn)\n",
    "        x = self.residual3(x , lambda x: self.ffn(x))\n",
    "        return x , (selfpresent , None)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self , x , enc_out , tgt_mask , src_mask , pastvalues):\n",
    "        new = []\n",
    "        for i , layer in enumerate(self.layers):\n",
    "            past = pastvalues[i] if pastvalues else None\n",
    "            x , layergrp = layer(x , enc_out , tgt_mask , src_mask , pastlayergroup = past)\n",
    "            new.append(layergrp)\n",
    "\n",
    "        return self.norm(x) , new\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.project = nn.Parameter(torch.randn(vocab_size , embed_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(vocab_size))\n",
    "        nn.init.xavier_uniform_(self.project)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        # self.project = nn.Linear(embed_dim , vocab_size)\n",
    "\n",
    "    def forward(self , x): #(batch , seq , embed)\n",
    "        return x @ self.project.T + self.bias\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self , encoder : Encoder , decoder : Decoder , src_emb , tgt_emb , src_pos , tgt_pos , proj_layer):\n",
    "        super().__init__()\n",
    "        self.encoder_ = encoder\n",
    "        self.decoder_ = decoder\n",
    "        self.src_emb = src_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer \n",
    "    \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder_(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask, src_mask, pastvalues=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder_(tgt, encoder_out, tgt_mask, src_mask, pastvalues)\n",
    "    \n",
    "    def projection(self, x):\n",
    "        return self.proj_layer(x)\n",
    "\n",
    "def build(src_vocab , tgt_vocab , src_seq , tgt_seq , embed_dim = 512 , num_head = 8 , num_layer = 6 , dropout =  float(0.1) , hid_dim = 2048):\n",
    "    src_embed = Embedding(embed_dim = embed_dim , vocab_size = src_vocab)\n",
    "    tgt_embed = Embedding(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    src_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = src_seq , dropout = dropout)\n",
    "    tgt_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = tgt_seq , dropout = dropout)\n",
    "\n",
    "    enc_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        enc_attn = MulticlassAttention(embed_dim , num_head , dropout )\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        enc_block = EncoderBlock(attention = enc_attn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        enc_blocks.append(enc_block)\n",
    "\n",
    "    dec_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        dec_selfAttn = MulticlassAttention(embed_dim , num_head , dropout )\n",
    "        dec_crossAttn = MulticlassAttention(embed_dim , num_head , dropout)\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        dec_block = DecoderBlock(selfAttention = dec_selfAttn , crossAttention = dec_crossAttn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        dec_blocks.append(dec_block)\n",
    "\n",
    "    encoder = Encoder(features = embed_dim , layers = nn.ModuleList(enc_blocks))\n",
    "    decoder = Decoder(features = embed_dim , layers = nn.ModuleList(dec_blocks))\n",
    "\n",
    "    proj_layer = Projection(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    transformer = Transformer(encoder = encoder , decoder = decoder , src_emb = src_embed , tgt_emb = tgt_embed , src_pos = src_pos , tgt_pos = tgt_pos , proj_layer = proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "        \n",
    "transformer = build(src_seq = 8 , tgt_seq = 8 , src_vocab = 50 , tgt_vocab = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee4e8d",
   "metadata": {},
   "source": [
    "all tests are generated by AI to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "452fa585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def kaiming_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)  \n",
    "                std = math.sqrt(2.0 / fan_in)\n",
    "                param.data.normal_(0, std)\n",
    "            else: \n",
    "                pass\n",
    "        elif 'bias' in name.lower() or name.endswith('b'):\n",
    "            if param.dim() == 1:  \n",
    "                param.data.fill_(0.0)\n",
    "\n",
    "def xavier_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)\n",
    "                fan_out = param.size(0)\n",
    "                std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "                param.data.normal_(0, std)\n",
    "            else:\n",
    "                pass\n",
    "        elif 'bias' in name.lower() or 'qb' in name or 'kb' in name or 'vb' in name:  \n",
    "            if param.dim() == 1:\n",
    "                param.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1dc6cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self , embed_dim , hid_dim , dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Parameter(torch.randn(hid_dim , embed_dim)) #(hid , embed)\n",
    "        self.l1bias = nn.Parameter(torch.randn(hid_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear2 = nn.Parameter(torch.randn(embed_dim , hid_dim)) #(embed , hid)\n",
    "        self.l2bias = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear1)\n",
    "        nn.init.xavier_uniform_(self.linear2)\n",
    "        nn.init.zeros_(self.l1bias)\n",
    "        nn.init.zeros_(self.l2bias)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self , x): #x -> (batch , seq , embed) \n",
    "        # batch , seq , hid\n",
    "        hidden = self.act(x @ self.linear1.T + self.l1bias) #(batch , seq , embed) @ (embed , hid) -> (batch , seq , hid)\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = hidden @ self.linear2.T + self.l2bias #(batch , seq , hid) @ \n",
    "        return  output \n",
    "    # we project to higher dims to have more features to be learned. \n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self , features , eps = 10 **-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        std  = x.std(dim = -1 , keepdim  = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self , features , dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # sublayer is layer we skip while doing connection\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size , embed_dim) # row , columns   \n",
    "\n",
    "    def forward(self , x):\n",
    "        out = self.embed(x) * math.sqrt(self.embed_dim)\n",
    "        return  out\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self , embed_dim , seq_len , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        position = torch.arange(0 , seq_len , dtype = torch.float).unsqueeze(1) \n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "\n",
    "        # (sin(position / 10000 ** (2i / embed-dim))) lets call 10000 ** (2i / embed-dim)) term\n",
    "        term = torch.exp(torch.arange(0 , embed_dim , 2).float() * -math.log(10000.0) / embed_dim)\n",
    "        pe[: , 0::2] = torch.sin(position * term)\n",
    "        pe[: , 1::2] = torch.cos(position * term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self , z):\n",
    "        # z -> (batch , seq_len , embed_dim)\n",
    "        return self.dropout(z + (self.pe[: , :z.shape[1] , :]).requires_grad_(False))\n",
    "\n",
    "def RoPE(x, sin, cos, start_pos=0):\n",
    "    seq = x.size(-2)\n",
    "    head_dim = x.size(-1)\n",
    "    num_head_x = x.size(1)  # This could be 8 (query) or 1 (key)\n",
    "    \n",
    "    embed_dim = sin.size(-1)\n",
    "    total_heads = embed_dim // head_dim  # This should be 8\n",
    "    \n",
    "    sin_sliced = sin[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    cos_sliced = cos[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    \n",
    "    sin_sliced = sin_sliced.view(seq, total_heads, head_dim)\n",
    "    cos_sliced = cos_sliced.view(seq, total_heads, head_dim)\n",
    "    \n",
    "    if total_heads != num_head_x:\n",
    "        sin_sliced = sin_sliced[:, :num_head_x, :]\n",
    "        cos_sliced = cos_sliced[:, :num_head_x, :]\n",
    "    \n",
    "    sin_sliced = sin_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    cos_sliced = cos_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    \n",
    "   \n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    \n",
    "    sin_even = sin_sliced[..., 0::2]\n",
    "    sin_odd = sin_sliced[..., 1::2]\n",
    "    cos_even = cos_sliced[..., 0::2]\n",
    "    cos_odd = cos_sliced[..., 1::2]\n",
    "    \n",
    "    rotated = torch.empty_like(x)\n",
    "    rotated[..., 0::2] = x_even * cos_even - x_odd * sin_even\n",
    "    rotated[..., 1::2] = x_even * sin_odd + x_odd * cos_odd\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "def RoPE_Embed(dim , seq):\n",
    "    \n",
    "    term = 1.0 / (10000 ** (torch.arange(0 , dim , 2).float() / dim))\n",
    "    seq_ = torch.arange(seq).float()\n",
    "\n",
    "    emb = torch.outer(seq_ , term)\n",
    "    emb = torch.cat([emb , emb] , dim = -1)\n",
    "\n",
    "    return emb.sin() , emb.cos()\n",
    "\n",
    "class MulticlassAttention(nn.Module):\n",
    "    def __init__(self , embed , num_head , dropout , max_seq_len = 5000):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed\n",
    "        self.num_head = num_head\n",
    "\n",
    "        assert embed % num_head == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.dim = embed // num_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        self.wq = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.qb = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.wk = nn.Parameter(torch.randn(embed , self.dim))\n",
    "        self.kb = nn.Parameter(torch.randn(self.dim))\n",
    "\n",
    "        self.wv = nn.Parameter(torch.randn(embed , self.dim))\n",
    "        self.vb = nn.Parameter(torch.randn(self.dim))\n",
    "\n",
    "\n",
    "\n",
    "        # self.wq = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "        # self.wv = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "        # self.wk = nn.Linear(embed_dim , embed_dim , bias = True)\n",
    "\n",
    "        \n",
    "        self.wo = nn.Parameter(torch.randn(embed , embed))\n",
    "        # self.wo = nn.Linear(embed_dim , embed_dim , bias = False)\n",
    "\n",
    "        for param in [self.wq, self.wk, self.wv, self.wo]:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "        for bias in [self.qb, self.kb, self.vb]:\n",
    "            nn.init.zeros_(bias)\n",
    "\n",
    "        self.register_buffer('sin', None)\n",
    "        self.register_buffer('cos', None)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def _precompute_rope(self, seq_len):\n",
    "        if self.sin is None or self.sin.size(0) < seq_len:\n",
    "            sin, cos = RoPE_Embed(self.embed_dim, seq_len)\n",
    "            self.register_buffer('sin', sin)\n",
    "            self.register_buffer('cos', cos)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout, pastlayer=None):\n",
    "        d_k = query.shape[-1]\n",
    "\n",
    "        if pastlayer is not None:\n",
    "            past_k, past_v = pastlayer\n",
    "            # Past first, then current\n",
    "            key = torch.cat([past_k, key], dim=-2)\n",
    "            value = torch.cat([past_v, value], dim=-2)\n",
    "\n",
    "        present = (key, value)\n",
    "\n",
    "        # Compute attention scores\n",
    "        #(batch , num_head , seq , d_k(dim)) @ (batch , 1 , d_k(dim) , seq) \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        print(f\"  scores shape: {scores.shape}\")\n",
    "\n",
    "        if mask is not None:\n",
    "            print(f\"  original mask shape: {mask.shape}\")\n",
    "            \n",
    "            current_kv_len = key.size(-2)\n",
    "            current_q_len = query.size(-2)\n",
    "            \n",
    "            # CRITICAL FIX: Create proper mask for current sequence length\n",
    "            if pastlayer is not None:\n",
    "                # Create causal mask for the current total sequence length\n",
    "                causal_mask = torch.tril(torch.ones(current_kv_len, current_kv_len, device=scores.device))\n",
    "\n",
    "                causal_mask = causal_mask[-current_q_len:, :].unsqueeze(0).unsqueeze(0)\n",
    "                if mask.size(-1) >= current_kv_len:\n",
    "                    \n",
    "                    padding_mask = mask[..., -current_kv_len:]\n",
    "                else:\n",
    "                    padding_mask = torch.ones_like(mask[..., :current_kv_len])\n",
    "                causal_mask = causal_mask.expand(mask.size(0), -1, -1, -1)\n",
    "                \n",
    "                # Combine masks\n",
    "                mask = causal_mask * padding_mask\n",
    "                print(f\"  combined mask shape: {mask.shape}\")\n",
    "                \n",
    "            else:\n",
    "                if mask.size(-1) != current_kv_len:\n",
    "                    mask = mask[..., :current_kv_len]\n",
    "                \n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        max_ = torch.max(scores , dim = -1 , keepdim = True)[0]\n",
    "        sc = torch.exp(scores - max_)\n",
    "        scores = sc / torch.sum(sc , dim = -1 , keepdim = True)\n",
    "        # scores = scores.softmax(dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        return (scores @ value), present\n",
    "    \n",
    "    def forward(self , q , k , v , mask = None , pastlayer = None , start_pos = 0):\n",
    "        batch = q.size(0)\n",
    "        \n",
    "\n",
    "        query = q @ self.wq + self.qb # (batch , seq_len , embed_dim)\n",
    "        key = k @ self.wk + self.kb\n",
    "        value = v @ self.wv + self.vb\n",
    "\n",
    "        \n",
    "        # (batch , seq_len , d_model(embed_dim)) -> (batch , seq , num_head , d_k(dim))\n",
    "        # transpose (batch , seq , num_head , d_k(dim)) -> (batch , num_head , seq , d_k(dim))\n",
    "        # by transposing it can work parallelly like take a batch , take head then give the seq (words)\n",
    "        query = query.reshape(batch , q.size(1) , self.num_head , self.dim).permute(0 , 2 , 1 , 3)\n",
    "        key = key.unsqueeze(1)\n",
    "        value = value.unsqueeze(1)\n",
    "\n",
    "        # key = key.reshape(batch , k.size(1) , 1 , self.dim).permute(0 , 2 , 1 , 3)\n",
    "        # value = value.reshape(batch , v.size(1) , 1 , self.dim).permute(0 , 2 , 1 , 3)\n",
    "\n",
    "        total_seq = max(q.size(1) + start_pos, \n",
    "                       k.size(1) + (pastlayer[0].size(2) if pastlayer else 0))\n",
    "        self._precompute_rope(total_seq)\n",
    "        query = RoPE(query, self.sin, self.cos, start_pos)\n",
    "        key = RoPE(key, self.sin, self.cos, 0)\n",
    "\n",
    "        out , pastlayer_ = MulticlassAttention.attention(query , key , value , mask , self.dropout , pastlayer)\n",
    "        # (batch , num_head , seq , d_k) -> # (batch , seq , num_head , d_k) -> (batch , seq , d_model)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch , q.size(1) , self.embed_dim)\n",
    "   \n",
    "        return out @ self.wo , pastlayer_# (batch , seq , d_model) \n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , attention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.ffn = ffn\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "\n",
    "        \n",
    "    def forward(self , x , mask):\n",
    "        \n",
    "        selfattn , _ = self.attention(x , x , x , mask)\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda x: self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.layer = layers\n",
    "\n",
    "    def forward(self , x , mask):\n",
    "        for layer in self.layer:\n",
    "            x = layer(x , mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self , selfAttention , crossAttention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = selfAttention\n",
    "        self.ffn = ffn\n",
    "        self.cross = crossAttention\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "        self.residual3 = Residual(feat , dropout)\n",
    "\n",
    "    def forward(self , x , enoderOutput , selfmask , crossmask , pastlayergroup):\n",
    "        selfpast , _ = pastlayergroup if pastlayergroup is not None else (None , None)\n",
    "        \n",
    "        selfattn , selfpresent = self.attention(x , x , x , selfmask , pastlayer = selfpast)\n",
    "        crossattn , crosspresent = self.cross(x , enoderOutput , enoderOutput , crossmask , pastlayer = None)\n",
    "\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda _: crossattn)\n",
    "        x = self.residual3(x , lambda x: self.ffn(x))\n",
    "        return x , (selfpresent , None)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self , x , enc_out , tgt_mask , src_mask , pastvalues):\n",
    "        new = []\n",
    "        for i , layer in enumerate(self.layers):\n",
    "            past = pastvalues[i] if pastvalues else None\n",
    "            x , layergrp = layer(x , enc_out , tgt_mask , src_mask , pastlayergroup = past)\n",
    "            new.append(layergrp)\n",
    "\n",
    "        return self.norm(x) , new\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.project = nn.Parameter(torch.randn(vocab_size , embed_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(vocab_size))\n",
    "        nn.init.xavier_uniform_(self.project)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        # self.project = nn.Linear(embed_dim , vocab_size)\n",
    "\n",
    "    def forward(self , x): #(batch , seq , embed)\n",
    "        return x @ self.project.T + self.bias\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self , encoder : Encoder , decoder : Decoder , src_emb , tgt_emb , src_pos , tgt_pos , proj_layer):\n",
    "        super().__init__()\n",
    "        self.encoder_ = encoder\n",
    "        self.decoder_ = decoder\n",
    "        self.src_emb = src_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer \n",
    "    \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder_(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask, src_mask, pastvalues=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder_(tgt, encoder_out, tgt_mask, src_mask, pastvalues)\n",
    "    \n",
    "    def projection(self, x):\n",
    "        return self.proj_layer(x)\n",
    "\n",
    "def build(src_vocab , tgt_vocab , src_seq , tgt_seq , embed_dim = 512 , num_head = 8 , num_layer = 6 , dropout =  float(0.1) , hid_dim = 2048):\n",
    "    src_embed = Embedding(embed_dim = embed_dim , vocab_size = src_vocab)\n",
    "    tgt_embed = Embedding(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    src_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = src_seq , dropout = dropout)\n",
    "    tgt_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = tgt_seq , dropout = dropout)\n",
    "\n",
    "    enc_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        enc_attn = MulticlassAttention(embed_dim , num_head , dropout )\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        enc_block = EncoderBlock(attention = enc_attn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        enc_blocks.append(enc_block)\n",
    "\n",
    "    dec_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        dec_selfAttn = MulticlassAttention(embed_dim , num_head , dropout )\n",
    "        dec_crossAttn = MulticlassAttention(embed_dim , num_head , dropout)\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        dec_block = DecoderBlock(selfAttention = dec_selfAttn , crossAttention = dec_crossAttn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        dec_blocks.append(dec_block)\n",
    "\n",
    "    encoder = Encoder(features = embed_dim , layers = nn.ModuleList(enc_blocks))\n",
    "    decoder = Decoder(features = embed_dim , layers = nn.ModuleList(dec_blocks))\n",
    "\n",
    "    proj_layer = Projection(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    transformer = Transformer(encoder = encoder , decoder = decoder , src_emb = src_embed , tgt_emb = tgt_embed , src_pos = src_pos , tgt_pos = tgt_pos , proj_layer = proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "        \n",
    "transformer = build(src_seq = 8 , tgt_seq = 8 , src_vocab = 50 , tgt_vocab = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da358fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE(x, sin, cos, start_pos=0):\n",
    "    seq = x.size(-2)\n",
    "    head_dim = x.size(-1)\n",
    "    num_head_x = x.size(1)  # This could be 8 (query) or 1 (key)\n",
    "    \n",
    "    embed_dim = sin.size(-1)\n",
    "    total_heads = embed_dim // head_dim  # This should be 8\n",
    "    \n",
    "    sin_sliced = sin[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    cos_sliced = cos[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    \n",
    "    sin_sliced = sin_sliced.view(seq, total_heads, head_dim)\n",
    "    cos_sliced = cos_sliced.view(seq, total_heads, head_dim)\n",
    "    \n",
    "    if total_heads != num_head_x:\n",
    "        sin_sliced = sin_sliced[:, :num_head_x, :]\n",
    "        cos_sliced = cos_sliced[:, :num_head_x, :]\n",
    "    \n",
    "    sin_sliced = sin_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    cos_sliced = cos_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    \n",
    "   \n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    \n",
    "    sin_even = sin_sliced[..., 0::2]\n",
    "    sin_odd = sin_sliced[..., 1::2]\n",
    "    cos_even = cos_sliced[..., 0::2]\n",
    "    cos_odd = cos_sliced[..., 1::2]\n",
    "    \n",
    "    rotated = torch.empty_like(x)\n",
    "    rotated[..., 0::2] = x_even * cos_even - x_odd * sin_even\n",
    "    rotated[..., 1::2] = x_even * sin_odd + x_odd * cos_odd\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "def RoPE_Embed(dim, seq):\n",
    "    term = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    \n",
    "    seq_ = torch.arange(seq).float()\n",
    "    \n",
    "    emb = torch.outer(seq_, term)\n",
    "    \n",
    "    emb = torch.cat([emb, emb], dim=-1)\n",
    "    \n",
    "    return emb.sin(), emb.cos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37715f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
