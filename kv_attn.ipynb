{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f9c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8006558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ffn\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self , hid_dim , embed , dropout : float):\n",
    "        super().__init__()        # we project in high dims\n",
    "        #we will use manual instead of linear projection , we will be using transpose to match architecture of eco-system\n",
    "        self.ln1 = nn.Parameter(torch.randn(hid_dim , embed)) #(hid_dim , embed)\n",
    "        self.ln1B = nn.Parameter(torch.randn(hid_dim))\n",
    "\n",
    "        self.ln2 = nn.Parameter(torch.randn(embed , hid_dim))\n",
    "        self.ln2B = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ln1)\n",
    "        nn.init.zeros_(self.ln1B)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.ln2)\n",
    "        nn.init.zeros_(self.ln2B)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self , x):\n",
    "        # x.shape = [batch , seq , embed]\n",
    "        hidden = self.act(x @ self.ln1.T + self.ln1B)# [batch , seq , hid]\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = hidden @ self.ln2.T + self.ln2B\n",
    "\n",
    "        return output\n",
    "    \n",
    "class LayerNormalizaton(nn.Module):\n",
    "    def __init__(self , features , eps = 1e-4):\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.randn(features))\n",
    "        self.bias = nn.Parameter(torch.randn(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        std = x.std(dim = -1 , keepdim = True)\n",
    "\n",
    "        return (x - mean) / (std + self.eps) * self.alpha + self.bias\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self , features , dropout : float):\n",
    "        super().__init__()\n",
    "        self.layer1 = LayerNormalizaton(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer1(x)))\n",
    "    \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self , embed , vocab):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab , embed)\n",
    "        self.embed_dim = embed\n",
    "    \n",
    "    def forward(self , x):\n",
    "        output = self.embed(x) * math.sqrt(self.embed_dim)\n",
    "        return output\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self , embed , seq_len , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pos = torch.arange(0 , seq_len , dtype = torch.float32).unsqueeze(1)\n",
    "        term = torch.exp(torch.arange(0 , embed , 2).float() * -math.log(10000) / embed)\n",
    "        pe = torch.zeros(seq_len , embed)\n",
    "\n",
    "        pe[: , 0::2] = torch.sin(pos * term)\n",
    "        pe[: , 1::2] = torch.cos(pos * term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe' , pe)\n",
    "\n",
    "    def forward(self , x):\n",
    "        return self.dropout(x + (self.pe[: , :x.shape[1] , :].requires_grad_(False)))\n",
    "    \n",
    "class MulticlassAttention(nn.Module):\n",
    "    def __init__(self , embed , num_head , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "        self.num_head = num_head\n",
    "        assert (embed % num_head == 0) , \"dusra try kr\" \n",
    "\n",
    "        self.dk = embed // num_head\n",
    "\n",
    "        self.q = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.q_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.k = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.k_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.v = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.v_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.o = nn.Parameter(torch.randn(embed , embed))\n",
    "        self.o_bias = nn.Parameter(torch.randn(embed))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        for name in [self.q , self.k , self.v , self.o]:\n",
    "            nn.init.xavier_uniform_(name)\n",
    "        for name in [self.q_bias , self.k_bias , self.v_bias , self.o_bias]:\n",
    "            nn.init.zeros_(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(q , k , v , mask , dropout , pastlayer):\n",
    "        dk = q.size(-1)\n",
    "\n",
    "        if pastlayer is not None:\n",
    "            k_ , v_ = pastlayer\n",
    "            k = torch.cat([k_ , k] , dim = -2)\n",
    "            v = torch.cat([v_ , v] , dim = -2)\n",
    "        \n",
    "        present = (k , v)\n",
    "        \n",
    "        scores = (q @ k.transpose(-2 , -1)) / math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            print(f\"  original mask shape: {mask.shape}\")\n",
    "            \n",
    "            current_kv_len = k.size(-2)\n",
    "            current_q_len = q.size(-2)\n",
    "            \n",
    "            if pastlayer is not None:\n",
    "                causal_mask = torch.tril(torch.ones(current_kv_len, current_kv_len, device=scores.device))\n",
    "\n",
    "                causal_mask = causal_mask[-current_q_len:, :].unsqueeze(0).unsqueeze(0)\n",
    "                if mask.size(-1) >= current_kv_len:\n",
    "                    \n",
    "                    padding_mask = mask[..., -current_kv_len:]\n",
    "                else:\n",
    "                    padding_mask = torch.ones_like(mask[..., :current_kv_len])\n",
    "                causal_mask = causal_mask.expand(mask.size(0), -1, -1, -1)\n",
    "                \n",
    "                # Combine masks\n",
    "                mask = causal_mask * padding_mask\n",
    "                print(f\"  combined mask shape: {mask.shape}\")\n",
    "                \n",
    "            else:\n",
    "                if mask.size(-1) != current_kv_len:\n",
    "                    mask = mask[..., :current_kv_len]\n",
    "                \n",
    "                if mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(1)\n",
    "            \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        max_ = torch.max(scores , dim = -1 , keepdim = True)[0]\n",
    "        sc = torch.exp(scores - max_)\n",
    "        scores = sc / torch.sum(sc , dim = -1 , keepdim = True)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        \n",
    "        return (scores @ v), present\n",
    "        \n",
    "\n",
    "    def forward(self , q , k , v , mask = None , pastlayer = None): #batch , seq , embed\n",
    "        q = q @ self.q + self.q_bias\n",
    "        k = k @ self.k + self.k_bias\n",
    "        v = v @ self.v + self.v_bias\n",
    "\n",
    "        batch = q.size(0)\n",
    "        seq = q.size(1)\n",
    "        k_seq = k.size(1)\n",
    "\n",
    "        query = q.view(batch , seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "        key = k.view(batch , k_seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "        value = v.view(batch , k_seq , self.num_head , self.dk).permute(0 , 2 , 1 , 3)\n",
    "\n",
    "        attn , present = MulticlassAttention.attention(query , key , value , mask , self.dropout , pastlayer)\n",
    "\n",
    "        attn = attn.permute(0, 2, 1, 3).contiguous().view(batch , seq , self.embed)\n",
    "\n",
    "        return attn @ self.o + self.o_bias , present\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , attn , ffn , feat , dropout : float):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.ffn = ffn\n",
    "        self.res1 = Residual(feat , dropout)\n",
    "        self.res2 = Residual(feat , dropout)\n",
    "\n",
    "    def forward(self , x , mask):\n",
    "        selfattn , _ = self.attn(x , x , x , mask , pastlayer = None)\n",
    "        x = self.res1(x , lambda _: selfattn)\n",
    "        x = self.res2(x , lambda a: self.ffn(a))\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self , feat , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalizaton(feat)\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self , x , mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x , mask)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self , selfattn , crossattn , ffn , feat , dropout : float):\n",
    "        super().__init__()\n",
    "        self.selfattn = selfattn\n",
    "        self.crossattn = crossattn\n",
    "        self.ffn = ffn\n",
    "        self.res1 = Residual(feat , dropout)\n",
    "        self.res2 = Residual(feat , dropout)\n",
    "        self.res3 = Residual(feat , dropout)\n",
    "    \n",
    "    def forward(self , x , encoderout , selfmask , crossmask , pastlayer):\n",
    "        attnpresent , _ = pastlayer if pastlayer else (None , None)\n",
    "\n",
    "        selfattn , selfpresent = self.selfattn(x , x , x , selfmask , attnpresent)\n",
    "        crossattn , _ = self.crossattn(x , encoderout , encoderout , crossmask , pastlayer = None)\n",
    "\n",
    "        x = self.res1(x , lambda _ : selfattn)\n",
    "        x = self.res2(x , lambda _ : crossattn)\n",
    "        x = self.res3(x , lambda x : self.ffn(x))\n",
    "\n",
    "        return x , (selfpresent , None)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , feat , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalizaton(feat)\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self , x , enc , tgt_mask , src_mask , pastvalues):\n",
    "        new = []\n",
    "\n",
    "        for i , layer in enumerate(self.layers):\n",
    "            past = pastvalues[i] if pastvalues else None\n",
    "            x , layerpast = layer(x , enc , tgt_mask , src_mask , past)\n",
    "\n",
    "            new.append(layerpast)\n",
    "        \n",
    "        return self.norm(x) , new\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self , embed , vocab):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Parameter(torch.randn(vocab , embed))\n",
    "        self.bias = nn.Parameter(torch.randn(vocab))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self , x):\n",
    "        out = x @ self.linear.T + self.bias\n",
    "        return out\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self , encoder : Encoder , decoder : Decoder , src_emb , tgt_emb , src_pos , tgt_pos , proj_layer):\n",
    "        super().__init__()\n",
    "        self.encoder_ = encoder\n",
    "        self.decoder_ = decoder\n",
    "        self.src_emb = src_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer \n",
    "    \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder_(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask, src_mask, pastvalues=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder_(tgt, encoder_out, tgt_mask, src_mask, pastvalues)\n",
    "    \n",
    "    def projection(self, x):\n",
    "        return self.proj_layer(x)\n",
    "\n",
    "def build(src_vocab , tgt_vocab , src_seq , tgt_seq , embed = 512 , num_head = 8 , num_layer = 6 , dropout =  float(0.1) , hid_dim = 2048):\n",
    "    src_embed = Embedding(embed = embed , vocab = src_vocab)\n",
    "    tgt_embed = Embedding(embed = embed , vocab = tgt_vocab)\n",
    "\n",
    "    src_pos = PositionalEmbedding(embed = embed , seq_len = src_seq , dropout = dropout)\n",
    "    tgt_pos = PositionalEmbedding(embed = embed , seq_len = tgt_seq , dropout = dropout)\n",
    "\n",
    "    enc_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        enc_attn = MulticlassAttention(embed , num_head , dropout )\n",
    "        ffn = FFN(hid_dim , embed , dropout)\n",
    "        enc_block = EncoderBlock(attn = enc_attn , ffn = ffn , feat = embed , dropout = dropout)\n",
    "        enc_blocks.append(enc_block)\n",
    "\n",
    "    dec_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        dec_selfAttn = MulticlassAttention(embed , num_head , dropout)\n",
    "        dec_crossAttn = MulticlassAttention(embed , num_head , dropout)\n",
    "        ffn = FFN(hid_dim , embed , dropout)\n",
    "        dec_block = DecoderBlock(selfattn = dec_selfAttn , crossattn = dec_crossAttn , ffn = ffn , feat = embed , dropout = dropout)\n",
    "        dec_blocks.append(dec_block)\n",
    "\n",
    "    encoder = Encoder(feat = embed , layers = nn.ModuleList(enc_blocks))\n",
    "    decoder = Decoder(feat = embed , layers = nn.ModuleList(dec_blocks))\n",
    "\n",
    "    proj_layer = Projection(embed = embed , vocab = tgt_vocab)\n",
    "\n",
    "    transformer = Transformer(encoder = encoder , decoder = decoder , src_emb = src_embed , tgt_emb = tgt_embed , src_pos = src_pos , tgt_pos = tgt_pos , proj_layer = proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "        \n",
    "transformer = build(src_vocab = 50 , tgt_vocab = 50 , src_seq = 8 , tgt_seq = 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def kaiming_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)  \n",
    "                std = math.sqrt(2.0 / fan_in)\n",
    "                param.data.normal_(0, std)\n",
    "            else: \n",
    "                pass\n",
    "        elif 'bias' in name.lower() or name.endswith('b'):\n",
    "            if param.dim() == 1:  \n",
    "                param.data.fill_(0.0)\n",
    "\n",
    "def xavier_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)\n",
    "                fan_out = param.size(0)\n",
    "                std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "                param.data.normal_(0, std)\n",
    "            else:\n",
    "                pass\n",
    "        elif 'bias' in name.lower() or 'qb' in name or 'kb' in name or 'vb' in name:  \n",
    "            if param.dim() == 1:\n",
    "                param.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baee4e8d",
   "metadata": {},
   "source": [
    "all tests are generated by AI to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e7b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: PREFILL (Processing the prompt)\n",
      "============================================================\n",
      "Input shape: torch.Size([1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "Encoder output shape: torch.Size([1, 5, 512])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 5, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "Step 1 Output shape: torch.Size([1, 5, 512])\n",
      "Cache size: 5 tokens\n",
      "\n",
      "============================================================\n",
      "STEP 2: GENERATION (The 'Aha!' Moment)\n",
      "============================================================\n",
      "Step 2 Input shape: torch.Size([1, 1])\n",
      "Inference mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 6])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 6])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "\n",
      "--- TEST RESULTS ---\n",
      "Step 2 Output Shape: torch.Size([1, 1, 512])\n",
      "Cache before: 5 tokens\n",
      "Cache after: 6 tokens\n",
      "\n",
      "✅ SUCCESS: Cache grew from 5 to 6 tokens!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Assuming your model is already built and initialized\n",
    "batch = 1\n",
    "seq_len = 5\n",
    "\n",
    "# Create the model first\n",
    "model = build(\n",
    "    src_vocab=30,\n",
    "    tgt_vocab=30,\n",
    "    src_seq=seq_len,\n",
    "    tgt_seq=seq_len,\n",
    "    embed=512,\n",
    "    num_head=8,\n",
    "    num_layer=6,\n",
    "    dropout=0.1,\n",
    "    hid_dim=2048\n",
    ")\n",
    "\n",
    "# Create necessary masks\n",
    "src_mask = torch.ones(batch, 1, 1, seq_len)  # Encoder mask (no padding)\n",
    "tgt_mask_full = torch.tril(torch.ones(seq_len, seq_len)).view(1, 1, seq_len, seq_len)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: PREFILL (Processing the prompt)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- STEP 1: PREFILL (The Prompt) ---\n",
    "input_5 = torch.randint(0, 30, (batch, seq_len))\n",
    "print(f\"Input shape: {input_5.shape}\")\n",
    "\n",
    "enc_out = model.encode(input_5, src_mask)\n",
    "print(f\"Encoder output shape: {enc_out.shape}\")\n",
    "\n",
    "output_5, cache_5 = model.decode(\n",
    "    input_5, \n",
    "    enc_out, \n",
    "    tgt_mask_full, \n",
    "    src_mask, \n",
    "    pastvalues=None\n",
    ")\n",
    "\n",
    "print(f\"Step 1 Output shape: {output_5.shape}\")\n",
    "print(f\"Cache size: {cache_5[0][0][0].shape[2]} tokens\")  # Should be 5\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: GENERATION (The 'Aha!' Moment)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- STEP 2: GENERATION ---\n",
    "# Take the last token from output_5 OR a random token\n",
    "logits = model.projection(output_5[:, -1:, :]) # Shape: (1, 1, 30)\n",
    "\n",
    "# 2. Now pick the token from the 30 possibilities\n",
    "input_1 = logits.argmax(dim=-1) # Shape: (1, 1)  # Use model's prediction\n",
    "# OR for testing: input_1 = torch.randint(0, 30, (batch, 1))\n",
    "\n",
    "print(f\"Step 2 Input shape: {input_1.shape}\")\n",
    "\n",
    "# Mask for single token - allows attending to ALL past tokens (5 previous + current = 6)\n",
    "# Shape: [batch, 1, 1, current_total_length]\n",
    "current_total_length = cache_5[0][0][0].shape[2] + 1  # 5 + 1 = 6\n",
    "tgt_mask_inference = torch.ones(batch, 1, 1, current_total_length)\n",
    "print(f\"Inference mask shape: {tgt_mask_inference.shape}\")\n",
    "\n",
    "# Decode ONLY the new token with cache\n",
    "output_1, cache_6 = model.decode(\n",
    "    input_1,                # Single token [1, 1]\n",
    "    enc_out,                # Same encoder output\n",
    "    tgt_mask_inference,      # Mask for attending to all 6 tokens\n",
    "    src_mask,                # Same source mask\n",
    "    pastvalues=cache_5       # Cache from step 1\n",
    ")\n",
    "\n",
    "print(f\"\\n--- TEST RESULTS ---\")\n",
    "print(f\"Step 2 Output Shape: {output_1.shape}\")     # Should be (1, 1, 512)\n",
    "print(f\"Cache before: {cache_5[0][0][0].shape[2]} tokens\")  # 5\n",
    "print(f\"Cache after: {cache_6[0][0][0].shape[2]} tokens\")   # Should be 6\n",
    "\n",
    "# Verify cache grew correctly\n",
    "if cache_6[0][0][0].shape[2] == cache_5[0][0][0].shape[2] + 1:\n",
    "    print(\"\\n✅ SUCCESS: Cache grew from 5 to 6 tokens!\")\n",
    "else:\n",
    "    print(\"\\n❌ FAILURE: Cache didn't grow correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5db45960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 7])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 7])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 8])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 8])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 9])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 9])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 10])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 10])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 11])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 11])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 12])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 12])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 13])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 13])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 14])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 14])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 15])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 15])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "  original mask shape: torch.Size([1, 1, 1, 16])\n",
      "  combined mask shape: torch.Size([1, 1, 1, 16])\n",
      "  original mask shape: torch.Size([1, 1, 1, 5])\n",
      "Generated Token IDs: [22, 22, 22, 22, 22, 0, 0, 22, 22, 6]\n"
     ]
    }
   ],
   "source": [
    "# Starting prompt (already processed in Step 1)\n",
    "current_input = input_1 \n",
    "current_cache = cache_6\n",
    "generated_tokens = []\n",
    "\n",
    "for _ in range(10):\n",
    "    # 1. Get the current total length for the mask\n",
    "    total_len = current_cache[0][0][0].shape[2] + 1\n",
    "    mask = torch.ones(1, 1, 1, total_len)\n",
    "    \n",
    "    # 2. Forward pass\n",
    "    out, next_cache = model.decode(current_input, enc_out, mask, src_mask, current_cache)\n",
    "    \n",
    "    # 3. Project to vocab and pick next token\n",
    "    logits = model.projection(out)\n",
    "    next_token = logits.argmax(dim=-1)\n",
    "    \n",
    "    # 4. Update for next iteration\n",
    "    generated_tokens.append(next_token.item())\n",
    "    current_input = next_token\n",
    "    current_cache = next_cache\n",
    "\n",
    "print(f\"Generated Token IDs: {generated_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "452fa585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def kaiming_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)  \n",
    "                std = math.sqrt(2.0 / fan_in)\n",
    "                param.data.normal_(0, std)\n",
    "            else: \n",
    "                pass\n",
    "        elif 'bias' in name.lower() or name.endswith('b'):\n",
    "            if param.dim() == 1:  \n",
    "                param.data.fill_(0.0)\n",
    "\n",
    "def xavier_initializer(self):\n",
    "    for name, param in self.named_parameters():\n",
    "        if 'weight' in name.lower() or 'wq' in name or 'wk' in name or 'wv' in name or 'wo' in name:\n",
    "            if param.dim() > 1:  \n",
    "                fan_in = param.size(1)\n",
    "                fan_out = param.size(0)\n",
    "                std = math.sqrt(2.0 / (fan_in + fan_out))\n",
    "                param.data.normal_(0, std)\n",
    "            else:\n",
    "                pass\n",
    "        elif 'bias' in name.lower() or 'qb' in name or 'kb' in name or 'vb' in name:  \n",
    "            if param.dim() == 1:\n",
    "                param.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6cd16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
