{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cb0714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only import torch and nn\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fea5932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write a softmax function\n",
    "def softmax(tensor , dim):\n",
    "    z_max = torch.max(tensor , dim = dim , keepdim = True).values\n",
    "    logits = tensor - z_max\n",
    "\n",
    "    n = torch.exp(logits)\n",
    "    sum = torch.sum(n , dim = dim , keepdim = True)\n",
    "\n",
    "    return n / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4edf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def crossEntropy(S , y_true):\n",
    "    batch_size = S.shape[0]\n",
    "    S = torch.clamp(S , 1e-12 , 1.0)\n",
    "    correct_class = S[torch.arange(batch_size) , y_true]\n",
    "    loss = -torch.log(correct_class)\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1af63477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.zeros(1 , 3)\n",
    "y = torch.tensor([2])\n",
    "s[0 , y] = 1.0\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "876e8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for epoch 0 | 0.0720861479640007\n",
      "loss for epoch 1 | 0.04807163029909134\n",
      "loss for epoch 2 | 0.0364856943488121\n",
      "loss for epoch 3 | 0.02953125163912773\n",
      "loss for epoch 4 | 0.024858413264155388\n",
      "loss for epoch 5 | 0.021489759907126427\n",
      "loss for epoch 6 | 0.018940292298793793\n",
      "loss for epoch 7 | 0.0169407669454813\n",
      "loss for epoch 8 | 0.015328949317336082\n",
      "loss for epoch 9 | 0.014001118019223213\n",
      "loss for epoch 10 | 0.012887735851109028\n",
      "loss for epoch 11 | 0.011940314434468746\n",
      "loss for epoch 12 | 0.011124026961624622\n",
      "loss for epoch 13 | 0.010413378477096558\n",
      "loss for epoch 14 | 0.009788942523300648\n",
      "loss for epoch 15 | 0.009235760197043419\n",
      "loss for epoch 16 | 0.008742287755012512\n",
      "loss for epoch 17 | 0.008299242705106735\n",
      "loss for epoch 18 | 0.007899348624050617\n",
      "loss for epoch 19 | 0.007536426652222872\n",
      "loss for epoch 20 | 0.007205695379525423\n",
      "loss for epoch 21 | 0.006902921479195356\n",
      "loss for epoch 22 | 0.006624658592045307\n",
      "loss for epoch 23 | 0.0063682482577860355\n",
      "loss for epoch 24 | 0.006130914203822613\n",
      "loss for epoch 25 | 0.005910784006118774\n",
      "loss for epoch 26 | 0.005705988500267267\n",
      "loss for epoch 27 | 0.0055150194093585014\n",
      "loss for epoch 28 | 0.005336431320756674\n",
      "loss for epoch 29 | 0.005169258452951908\n",
      "loss for epoch 30 | 0.0050121177919209\n",
      "loss for epoch 31 | 0.004864404909312725\n",
      "loss for epoch 32 | 0.004725098144263029\n",
      "loss for epoch 33 | 0.004593655001372099\n",
      "loss for epoch 34 | 0.004469294100999832\n",
      "loss for epoch 35 | 0.004351533483713865\n",
      "loss for epoch 36 | 0.004239952191710472\n",
      "loss for epoch 37 | 0.0041338293813169\n",
      "loss for epoch 38 | 0.004033044446259737\n",
      "loss for epoch 39 | 0.003936997149139643\n",
      "loss for epoch 40 | 0.0038454465102404356\n",
      "loss for epoch 41 | 0.003758032573387027\n",
      "loss for epoch 42 | 0.003674514591693878\n",
      "loss for epoch 43 | 0.0035947123542428017\n",
      "loss for epoch 44 | 0.0035183262079954147\n",
      "loss for epoch 45 | 0.003445055801421404\n",
      "loss for epoch 46 | 0.0033748408313840628\n",
      "loss for epoch 47 | 0.0033073220402002335\n",
      "loss for epoch 48 | 0.0032426181714981794\n",
      "loss for epoch 49 | 0.0031803103629499674\n",
      "final probs tensor([[0.0032, 0.9968]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "lr = 0.1\n",
    "\n",
    "dummy = torch.tensor([[2.0 , 3.0 , 4.0]])\n",
    "y_true = torch.tensor([1])\n",
    "\n",
    "weights = torch.randn(input_dim , output_dim , requires_grad = False)\n",
    "bias = torch.zeros(output_dim)\n",
    "\n",
    "# y = wx + b  , where x is the data\n",
    "for epoch in range(50):\n",
    "    z = torch.matmul(dummy , weights) + bias\n",
    "\n",
    "    exp_z = torch.exp(z - torch.max(z)) \n",
    "    S = exp_z / torch.sum(exp_z, dim=-1, keepdim=True)\n",
    "    loss = S[0 , y_true]\n",
    "    loss = -torch.log(loss)\n",
    "\n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0 , y_true] = 1.0\n",
    "\n",
    "    dz = S - y_hot\n",
    "\n",
    "    w_ = torch.matmul(dummy.t() , dz)\n",
    "    b_ = dz.sum(dim = 0)\n",
    "\n",
    "    weights = weights - lr * w_\n",
    "    bias = bias - lr * b_\n",
    "\n",
    "    print(f\"loss for epoch {epoch} | {loss.item()}\")\n",
    "\n",
    "print(f\"final probs {S}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fef290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 0.000001 | Probs: [9.7624911e-07 9.9999905e-01]\n",
      "Epoch  20 | Loss: 0.000001 | Probs: [9.7578834e-07 9.9999905e-01]\n",
      "Epoch  40 | Loss: 0.000001 | Probs: [9.7534735e-07 9.9999905e-01]\n",
      "Epoch  60 | Loss: 0.000001 | Probs: [9.7491215e-07 9.9999905e-01]\n",
      "Epoch  80 | Loss: 0.000001 | Probs: [9.7447708e-07 9.9999905e-01]\n",
      "==================================================\n",
      "Final probabilities: [9.7406269e-07 9.9999905e-01]\n",
      "Sum: 1.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Network architecture\n",
    "input_dim = 3\n",
    "hidden_dim = 4  # neurons in hidden layer\n",
    "output_dim = 2\n",
    "lr = 0.1\n",
    "\n",
    "# Data\n",
    "dummy = torch.tensor([[2.0, 3.0, 4.0]])  # [1, 3]\n",
    "y_true = torch.tensor([1])  # [1]\n",
    "\n",
    "# Initialize weights and biases for both layers\n",
    "W1 = torch.randn(input_dim, hidden_dim, requires_grad=False)  # [3, 4]\n",
    "b1 = torch.zeros(hidden_dim)  # [4]\n",
    "\n",
    "W2 = torch.randn(hidden_dim, output_dim, requires_grad=False)  # [4, 2]\n",
    "b2 = torch.zeros(output_dim)  # [2]\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    z1 = torch.matmul(dummy, W1) + b1 \n",
    "    h1 = F.relu(z1)  \n",
    "    \n",
    "    # Layer 2: hidden -> output\n",
    "    z2 = torch.matmul(h1, W2) + b2  \n",
    "    \n",
    "    # Softmax\n",
    "    z2_max = torch.max(z2, dim=-1, keepdim=True)[0]\n",
    "    exp_z2 = torch.exp(z2 - z2_max)\n",
    "    S = exp_z2 / torch.sum(exp_z2, dim=-1, keepdim=True)  # [1, 2]\n",
    "    \n",
    "    # Loss\n",
    "    loss = -torch.log(S[0, y_true])  \n",
    "    \n",
    "   \n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0, y_true] = 1.0\n",
    "    \n",
    "    dz2 = S - y_hot  # [1, 2]\n",
    "    \n",
    "    dW2 = torch.matmul(h1.T, dz2)  \n",
    "    db2 = dz2.sum(dim=0)  # [2]\n",
    "    \n",
    "    dh1 = torch.matmul(dz2, W2.T)  # [1, 2] @ [2, 4] = [1, 4]\n",
    "    \n",
    "    dz1 = dh1.clone()\n",
    "    dz1[z1 <= 0] = 0  \n",
    "\n",
    "    dW1 = torch.matmul(dummy.T, dz1)  # [3, 1] @ [1, 4] = [3, 4]\n",
    "    db1 = dz1.sum(dim=0)  # [4]\n",
    "    \n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d} | Loss: {loss.item():.6f} | Probs: {S[0].detach().numpy()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Final probabilities: {S[0].detach().numpy()}\")\n",
    "print(f\"Sum: {S.sum().item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae345859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 7.105299472808838\n",
      "loss 0.5982127785682678\n",
      "loss 0.5586756467819214\n",
      "loss 0.5229262113571167\n",
      "loss 0.4905574917793274\n",
      "loss 0.4612022936344147\n",
      "loss 0.4345317780971527\n",
      "loss 0.41025298833847046\n",
      "loss 0.388106107711792\n",
      "loss 0.3678610622882843\n",
      "loss 0.34931495785713196\n",
      "loss 0.3322887122631073\n",
      "loss 0.31662413477897644\n",
      "loss 0.3021819293498993\n",
      "loss 0.2888389825820923\n",
      "loss 0.27648669481277466\n",
      "loss 0.26502883434295654\n",
      "loss 0.2543801963329315\n",
      "loss 0.2444651871919632\n",
      "loss 0.23521646857261658\n",
      "loss 0.22657445073127747\n",
      "loss 0.218485489487648\n",
      "loss 0.210902139544487\n",
      "loss 0.20378142595291138\n",
      "loss 0.1970851868391037\n",
      "loss 0.19077888131141663\n",
      "loss 0.18483155965805054\n",
      "loss 0.17921511828899384\n",
      "loss 0.17390449345111847\n",
      "loss 0.16887637972831726\n",
      "loss 0.16411027312278748\n",
      "loss 0.15958721935749054\n",
      "loss 0.15529008209705353\n",
      "loss 0.15120309591293335\n",
      "loss 0.1473119556903839\n",
      "loss 0.14360356330871582\n",
      "loss 0.14006587862968445\n",
      "loss 0.13668812811374664\n",
      "loss 0.13345980644226074\n",
      "loss 0.13037189841270447\n",
      "loss 0.12741562724113464\n",
      "loss 0.1245831698179245\n",
      "loss 0.12186730653047562\n",
      "loss 0.11926107853651047\n",
      "loss 0.11675834655761719\n",
      "loss 0.11435327678918839\n",
      "loss 0.11204031109809875\n",
      "loss 0.10981470346450806\n",
      "loss 0.10767156630754471\n",
      "loss 0.10560654103755951\n",
      "loss 0.10361571609973907\n",
      "loss 0.1016952246427536\n",
      "loss 0.09984144568443298\n",
      "loss 0.09805118292570114\n",
      "loss 0.0963212177157402\n",
      "loss 0.0946488156914711\n",
      "loss 0.09303101897239685\n",
      "loss 0.09146533906459808\n",
      "loss 0.08994951844215393\n",
      "loss 0.08848118036985397\n",
      "loss 0.08705804497003555\n",
      "loss 0.0856783539056778\n",
      "loss 0.08433999866247177\n",
      "loss 0.08304131776094437\n",
      "loss 0.08178061246871948\n",
      "loss 0.0805562436580658\n",
      "loss 0.07936673611402512\n",
      "loss 0.07821054756641388\n",
      "loss 0.07708646357059479\n",
      "loss 0.07599322497844696\n",
      "loss 0.07492945343255997\n",
      "loss 0.07389414310455322\n",
      "loss 0.07288599759340286\n",
      "loss 0.07190423458814621\n",
      "loss 0.07094767689704895\n",
      "loss 0.07001550495624542\n",
      "loss 0.06910674273967743\n",
      "loss 0.06822062283754349\n",
      "loss 0.06735614687204361\n",
      "loss 0.06651273369789124\n",
      "loss 0.06568944454193115\n",
      "loss 0.06488584727048874\n",
      "loss 0.06410107016563416\n",
      "loss 0.06333456188440323\n",
      "loss 0.06258558481931686\n",
      "loss 0.06185365840792656\n",
      "loss 0.061138227581977844\n",
      "loss 0.06043870002031326\n",
      "loss 0.05975446477532387\n",
      "loss 0.05908530205488205\n",
      "loss 0.05843042954802513\n",
      "loss 0.057789623737335205\n",
      "loss 0.05716241896152496\n",
      "loss 0.056548282504081726\n",
      "loss 0.05594687908887863\n",
      "loss 0.055357810109853745\n",
      "loss 0.05478060618042946\n",
      "loss 0.054215189069509506\n",
      "loss 0.053661033511161804\n",
      "loss 0.05311780422925949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0517, 0.9483]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 4  \n",
    "output_dim = 2\n",
    "lr = 0.1\n",
    "\n",
    "# Data\n",
    "dummy = torch.tensor([[2.0, 3.0, 4.0]])  # [1, 3]\n",
    "y_true = torch.tensor([1])  # [1]\n",
    "\n",
    "W1 = torch.randn(input_dim, hidden_dim, requires_grad=False)  # [3, 4]\n",
    "b1 = torch.zeros(hidden_dim)  # [4]\n",
    "\n",
    "W2 = torch.randn(hidden_dim, output_dim, requires_grad=False)  # [4, 2]\n",
    "b2 = torch.zeros(output_dim)  # [2]\n",
    "\n",
    "for epoch in range(100):\n",
    "    z1 = torch.matmul(dummy , W1) + b1 # [1 , 4]\n",
    "    h1 = F.relu(z1)\n",
    "\n",
    "    z = torch.matmul(h1 , W2) + b2 # [1 , 2]\n",
    "\n",
    "    zexp = torch.exp(z - torch.max(z))\n",
    "    sum_ = torch.sum(zexp , dim = -1 , keepdim = True)\n",
    "\n",
    "    S = zexp / sum_ #[1 , 2]\n",
    "\n",
    "    loss = -torch.log(S[0 , y_true])\n",
    "\n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0 , y_true] = 1.0\n",
    "\n",
    "    dz2 = S - y_hot\n",
    "\n",
    "    dw2 = torch.matmul(h1.T , dz2)\n",
    "    db2 = dz2.sum(dim = 0)\n",
    "\n",
    "    dh1 = torch.matmul(dz2 , W2.T)\n",
    "    dh1[z1 <= 0] = 0\n",
    "\n",
    "    dw1 = torch.matmul(dummy.T , dh1)\n",
    "    db1 = dh1.sum(dim = 0)\n",
    "\n",
    "    W1 -= lr * dw1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dw2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    print(f\"loss {loss.item()}\")\n",
    "\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ea669e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962fdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
