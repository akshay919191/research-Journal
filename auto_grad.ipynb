{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb0714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will only import torch and nn\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fea5932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write a softmax function\n",
    "def softmax(tensor , dim):\n",
    "    z_max = torch.max(tensor , dim = dim , keepdim = True).values\n",
    "    logits = tensor - z_max\n",
    "\n",
    "    n = torch.exp(logits)\n",
    "    sum = torch.sum(n , dim = dim , keepdim = True)\n",
    "\n",
    "    return n / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4edf456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def crossEntropy(S , y_true):\n",
    "    batch_size = S.shape[0]\n",
    "    S = torch.clamp(S , 1e-12 , 1.0)\n",
    "    correct_class = S[torch.arange(batch_size) , y_true]\n",
    "    loss = -torch.log(correct_class)\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1af63477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.zeros(1 , 3)\n",
    "y = torch.tensor([2])\n",
    "s[0 , y] = 1.0\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876e8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 0 | 2.8380062580108643\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 1 | 0.05509262531995773\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 2 | 0.040240976959466934\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 3 | 0.03189440444111824\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 4 | 0.02649088017642498\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 5 | 0.022688308730721474\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 6 | 0.01985928975045681\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 7 | 0.01766856759786606\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 8 | 0.015920214354991913\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 9 | 0.014491206035017967\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 10 | 0.013300744816660881\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 11 | 0.012293191626667976\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 12 | 0.011429167352616787\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 13 | 0.010679804719984531\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 14 | 0.01002371497452259\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 15 | 0.009444228373467922\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 16 | 0.008928642608225346\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 17 | 0.008466759696602821\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 18 | 0.008050874806940556\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 19 | 0.007674143649637699\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 20 | 0.007331358268857002\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 21 | 0.007018101401627064\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 22 | 0.006730625871568918\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 23 | 0.006465969141572714\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 24 | 0.00622147461399436\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 25 | 0.005994908511638641\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 26 | 0.0057843406684696674\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 27 | 0.005588142201304436\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 28 | 0.005404866300523281\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 29 | 0.005233308300375938\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 30 | 0.005072323139756918\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 31 | 0.004921127576380968\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 32 | 0.004778638482093811\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 33 | 0.004644074477255344\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 34 | 0.004517132416367531\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 35 | 0.004396911710500717\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 36 | 0.004282811190932989\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 37 | 0.004174708854407072\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 38 | 0.0040718852542340755\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 39 | 0.003973918966948986\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 40 | 0.0038806896191090345\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 41 | 0.003791657043620944\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 42 | 0.003706700401380658\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 43 | 0.0036254599690437317\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 44 | 0.003547815140336752\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 45 | 0.0034733465872704983\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 46 | 0.0034019334707409143\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 47 | 0.0033333960454910994\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 48 | 0.003267674008384347\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "loss for epoch 49 | 0.0032043480314314365\n",
      "final probs tensor([[0.0032, 0.9968]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "lr = 0.1\n",
    "\n",
    "dummy = torch.tensor([[2.0 , 3.0 , 4.0]])\n",
    "y_true = torch.tensor([1])\n",
    "\n",
    "weights = torch.randn(input_dim , output_dim , requires_grad = False)\n",
    "bias = torch.zeros(output_dim)\n",
    "\n",
    "# y = wx + b  , where x is the data\n",
    "for epoch in range(50):\n",
    "    z = torch.matmul(dummy , weights) + bias\n",
    "\n",
    "    exp_z = torch.exp(z - torch.max(z)) \n",
    "    print(exp_z.shape)\n",
    "    sum_ = torch.sum(exp_z, dim=-1, keepdim=True)\n",
    "    print(sum_.shape)\n",
    "    S = exp_z / sum_\n",
    "    print(S.shape)\n",
    "    loss = S[0 , y_true]\n",
    "    print(loss.shape)\n",
    "    loss = -torch.log(loss)\n",
    "\n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0 , y_true] = 1.0\n",
    "\n",
    "    dz = S - y_hot\n",
    "\n",
    "    w_ = torch.matmul(dummy.t() , dz)\n",
    "    b_ = dz.sum(dim = 0)\n",
    "\n",
    "    weights = weights - lr * w_\n",
    "    bias = bias - lr * b_\n",
    "\n",
    "    print(f\"loss for epoch {epoch} | {loss.item()}\")\n",
    "\n",
    "print(f\"final probs {S}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae345859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 2.9964773654937744\n",
      "loss 0.6456148624420166\n",
      "loss 0.5570704340934753\n",
      "loss 0.49978017807006836\n",
      "loss 0.48107874393463135\n",
      "loss 0.46362969279289246\n",
      "loss 0.4469797909259796\n",
      "loss 0.4309196472167969\n",
      "loss 0.4153747856616974\n",
      "loss 0.4003027677536011\n",
      "loss 0.3856752812862396\n",
      "loss 0.37147197127342224\n",
      "loss 0.35767948627471924\n",
      "loss 0.34428858757019043\n",
      "loss 0.331293523311615\n",
      "loss 0.31869110465049744\n",
      "loss 0.30647963285446167\n",
      "loss 0.29465845227241516\n",
      "loss 0.2832268178462982\n",
      "loss 0.27218425273895264\n",
      "loss 0.26152968406677246\n",
      "loss 0.25126123428344727\n",
      "loss 0.2413761168718338\n",
      "loss 0.23187051713466644\n",
      "loss 0.22273963689804077\n",
      "loss 0.2139773964881897\n",
      "loss 0.20557697117328644\n",
      "loss 0.1975305825471878\n",
      "loss 0.18982942402362823\n",
      "loss 0.18246416747570038\n",
      "loss 0.17542463541030884\n",
      "loss 0.16870035231113434\n",
      "loss 0.16228045523166656\n",
      "loss 0.15615364909172058\n",
      "loss 0.15043987333774567\n",
      "loss 0.14510159194469452\n",
      "loss 0.14000476896762848\n",
      "loss 0.13513757288455963\n",
      "loss 0.1304900050163269\n",
      "loss 0.12605196237564087\n",
      "loss 0.12181389331817627\n",
      "loss 0.1177663505077362\n",
      "loss 0.11390022188425064\n",
      "loss 0.11020687967538834\n",
      "loss 0.10667777061462402\n",
      "loss 0.10330504924058914\n",
      "loss 0.10008088499307632\n",
      "loss 0.09699789434671402\n",
      "loss 0.0940491333603859\n",
      "loss 0.09122779220342636\n",
      "loss 0.08852753043174744\n",
      "loss 0.08594230562448502\n",
      "loss 0.0834662914276123\n",
      "loss 0.08109421283006668\n",
      "loss 0.07882072776556015\n",
      "loss 0.07664091885089874\n",
      "loss 0.07455015927553177\n",
      "loss 0.07254413515329361\n",
      "loss 0.07061849534511566\n",
      "loss 0.06876940280199051\n",
      "loss 0.06699306517839432\n",
      "loss 0.0652860477566719\n",
      "loss 0.0636448860168457\n",
      "loss 0.06206642463803291\n",
      "loss 0.060547757893800735\n",
      "loss 0.05908598378300667\n",
      "loss 0.057678382843732834\n",
      "loss 0.056322455406188965\n",
      "loss 0.055015843361616135\n",
      "loss 0.05375624820590019\n",
      "loss 0.05254146456718445\n",
      "loss 0.05136948823928833\n",
      "loss 0.05023835226893425\n",
      "loss 0.04914632439613342\n",
      "loss 0.048091575503349304\n",
      "loss 0.04707247018814087\n",
      "loss 0.04608749970793724\n",
      "loss 0.045135173946619034\n",
      "loss 0.04421403631567955\n",
      "loss 0.04332279786467552\n",
      "loss 0.042460113763809204\n",
      "loss 0.041624974459409714\n",
      "loss 0.04081607237458229\n",
      "loss 0.04003235325217247\n",
      "loss 0.03927280753850937\n",
      "loss 0.0385364294052124\n",
      "loss 0.037822335958480835\n",
      "loss 0.03712958097457886\n",
      "loss 0.03645741567015648\n",
      "loss 0.03580491989850998\n",
      "loss 0.03517137095332146\n",
      "loss 0.03455602005124092\n",
      "loss 0.03395824134349823\n",
      "loss 0.033377308398485184\n",
      "loss 0.03281259909272194\n",
      "loss 0.032263562083244324\n",
      "loss 0.03172947093844414\n",
      "loss 0.031209945678710938\n",
      "loss 0.03070438653230667\n",
      "loss 0.03021225519478321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0142, 0.9858],\n",
       "        [0.9628, 0.0372],\n",
       "        [0.0377, 0.9623]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 4  \n",
    "output_dim = 2\n",
    "lr = 0.1\n",
    "\n",
    "# Data\n",
    "dummy = torch.tensor([\n",
    "    [2.0, 3.0, 4.0],\n",
    "    [1.0, 0.5, 2.0],\n",
    "    [0.0, 1.0, 0.1]\n",
    "])\n",
    "y_true = torch.tensor([1, 0, 1]) # Target class for each example\n",
    "\n",
    "W1 = torch.randn(input_dim, hidden_dim, requires_grad=False)  # [3, 4]\n",
    "b1 = torch.zeros(hidden_dim)  # [4]\n",
    "\n",
    "W2 = torch.randn(hidden_dim, output_dim, requires_grad=False)  # [4, 2]\n",
    "b2 = torch.zeros(output_dim)  # [2]\n",
    "\n",
    "for epoch in range(100):\n",
    "    z1 = torch.matmul(dummy , W1) + b1 # [3 , 4]\n",
    "    h1 = F.relu(z1)\n",
    "\n",
    "    z = torch.matmul(h1 , W2) + b2 # [3 , 2]\n",
    "\n",
    "    zexp = torch.exp(z - torch.max(z)) #[3 , 2]\n",
    "    sum_ = torch.sum(zexp , dim = -1 , keepdim = True) # [3 , 1]\n",
    "\n",
    "    S = zexp / sum_ #[3 , 2]\n",
    "    #\n",
    "\n",
    "    loss = -torch.log(S[torch.arange(S.size(0)) , y_true]).mean()\n",
    "\n",
    "    y_hot = torch.zeros_like(S) #[3 , 2]\n",
    "    y_hot[torch.arange(S.size(0)) , y_true] = 1.0 #[3 , 2]\n",
    "\n",
    "    dz2 = ( S - y_hot ) / S.size(0) # loss with respect to weight #[3 , 2]\n",
    "\n",
    "    dw2 = torch.matmul(h1.T , dz2) # [4 , 2]\n",
    "    db2 = dz2.sum(dim = 0) #[1 , 2]\n",
    "\n",
    "    dh1 = torch.matmul(dz2 , W2.T) #[3 , 4]\n",
    "    dh1[z1 <= 0] = 0 #[3 , 4]\n",
    "\n",
    "    dw1 = torch.matmul(dummy.T , dh1) # [3 , 4]\n",
    "    db1 = dh1.sum(dim = 0) #[1 , 4]\n",
    "\n",
    "    W1 -= lr * dw1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dw2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    print(f\"loss {loss.item()}\")\n",
    "\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd530247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Loss: 0.699030 | Probs: [0.2038741  0.49706724 0.29905862]\n",
      "Epoch 10 | Loss: 0.174867 | Probs: [0.07584219 0.8395685  0.08458935]\n",
      "Epoch 20 | Loss: 0.087873 | Probs: [0.040824 0.915877 0.043299]\n",
      "Epoch 30 | Loss: 0.056565 | Probs: [0.02697902 0.9450054  0.02801557]\n",
      "Epoch 40 | Loss: 0.041005 | Probs: [0.01982892 0.95982426 0.02034688]\n",
      "==================================================\n",
      "Final probabilities: [0.01588207 0.9679364  0.01618156]\n",
      "Sum: 1.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_dim = 4\n",
    "output_dim = 3\n",
    "lr = 0.1\n",
    "\n",
    "vocab_size = 5\n",
    "embed_dim = 4  \n",
    "\n",
    "E = torch.randn(vocab_size, embed_dim, requires_grad=False)\n",
    "\n",
    "input_ids = torch.tensor([1, 3, 4])  # [3] \n",
    "\n",
    "y_true = torch.tensor([1])  # [1]\n",
    "\n",
    "\n",
    "weights = torch.randn(embed_dim, output_dim, requires_grad=False)  # [4, 3]\n",
    "bias = torch.zeros(output_dim)  # [3]\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    x = E[input_ids]  #  [3, 4] \n",
    "    \n",
    "    x_pooled = x.mean(dim=0, keepdim=True)  \n",
    "    \n",
    "    z = torch.matmul(x_pooled, weights) + bias  # [1, 3]\n",
    "    \n",
    "    # 4. Softmax\n",
    "    z_max = torch.max(z, dim=-1, keepdim=True)[0]\n",
    "    exp_z = torch.exp(z - z_max)\n",
    "    S = exp_z / torch.sum(exp_z, dim=-1, keepdim=True)  # [1, 3]\n",
    "    \n",
    "    # 5. Loss\n",
    "    loss = -torch.log(S[0, y_true])  # scalar\n",
    "    \n",
    "    # ===== BACKWARD PASS =====\n",
    "    # One-hot encoding\n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0, y_true] = 1.0\n",
    "    \n",
    "    # Gradient at output\n",
    "    dz = S - y_hot  # [1, 3]\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dW = torch.matmul(x_pooled.T, dz)  # [4, 1] @ [1, 3] = [4, 3]\n",
    "    db = dz.sum(dim=0)  # [3]\n",
    "    \n",
    "    # Gradient to pooled embedding\n",
    "    dx_pooled = torch.matmul(dz, weights.T)  # [1, 3] @ [3, 4] = [1, 4]\n",
    "    \n",
    "    # Gradient to individual word embeddings\n",
    "    # Since we averaged, gradient is distributed equally\n",
    "    dx = dx_pooled.expand_as(x) / x.shape[0]  # [3, 4]\n",
    "    \n",
    "    # Gradient to embedding matrix\n",
    "    dE = torch.zeros_like(E)\n",
    "    for i, idx in enumerate(input_ids):\n",
    "        dE[idx] += dx[i]  # Add gradient to each word's embedding\n",
    "    \n",
    "    # ===== UPDATE PARAMETERS =====\n",
    "    weights -= lr * dW\n",
    "    bias -= lr * db\n",
    "    E -= lr * dE\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d} | Loss: {loss.item():.6f} | Probs: {S[0].detach().numpy()}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"Final probabilities: {S[0].detach().numpy()}\")\n",
    "print(f\"Sum: {S.sum().item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb691d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Loss: 2.518187 | Probs: [0.40348715 0.08060565 0.5159072 ]\n",
      "Epoch  1 | Loss: 1.981625 | Probs: [0.3956377 0.137845  0.4665173]\n",
      "Epoch  2 | Loss: 1.575094 | Probs: [0.3745425  0.20698814 0.41846943]\n",
      "Epoch  3 | Loss: 1.268085 | Probs: [0.34607813 0.28137004 0.37255183]\n",
      "Epoch  4 | Loss: 1.035330 | Probs: [0.31483778 0.35510927 0.33005294]\n",
      "Epoch  5 | Loss: 0.857165 | Probs: [0.28383034 0.42436337 0.2918063 ]\n",
      "Epoch  6 | Loss: 0.719048 | Probs: [0.25472417 0.48721588 0.25806   ]\n",
      "Epoch  7 | Loss: 0.610512 | Probs: [0.22827505 0.5430727  0.22865225]\n",
      "Epoch  8 | Loss: 0.524082 | Probs: [0.20469598 0.5920985  0.20320556]\n",
      "Epoch  9 | Loss: 0.454394 | Probs: [0.18390748 0.63483244 0.18126003]\n",
      "Epoch 10 | Loss: 0.397555 | Probs: [0.16568956 0.67196107 0.16234945]\n",
      "Epoch 11 | Loss: 0.350702 | Probs: [0.14976776 0.70419335 0.14603896]\n",
      "Epoch 12 | Loss: 0.311703 | Probs: [0.13585982 0.73219895 0.13194115]\n",
      "Epoch 13 | Loss: 0.278946 | Probs: [0.1236991  0.75658095 0.11971997]\n",
      "Epoch 14 | Loss: 0.251200 | Probs: [0.11304484 0.77786654 0.10908862]\n",
      "Epoch 15 | Loss: 0.227516 | Probs: [0.10368542 0.79650974 0.09980491]\n",
      "Epoch 16 | Loss: 0.207151 | Probs: [0.09543792 0.8128967  0.09166538]\n",
      "Epoch 17 | Loss: 0.189522 | Probs: [0.08814573 0.82735455 0.08449976]\n",
      "Epoch 18 | Loss: 0.174164 | Probs: [0.08167545 0.8401588  0.07816564]\n",
      "Epoch 19 | Loss: 0.160706 | Probs: [0.07591398 0.85154223 0.07254382]\n",
      "Epoch 20 | Loss: 0.148848 | Probs: [0.07076539 0.86170006 0.06753445]\n",
      "Epoch 21 | Loss: 0.138345 | Probs: [0.06614836 0.87079793 0.0630537 ]\n",
      "Epoch 22 | Loss: 0.128999 | Probs: [0.06199382 0.8789752  0.0590309 ]\n",
      "Epoch 23 | Loss: 0.120643 | Probs: [0.05824303 0.8863506  0.0554064 ]\n",
      "Epoch 24 | Loss: 0.113141 | Probs: [0.0548459  0.89302444 0.05212963]\n",
      "Epoch 25 | Loss: 0.106380 | Probs: [0.0517596  0.8990829  0.04915757]\n",
      "Epoch 26 | Loss: 0.100263 | Probs: [0.0489474  0.90459913 0.04645351]\n",
      "Epoch 27 | Loss: 0.094710 | Probs: [0.04637769 0.9096363  0.043986  ]\n",
      "Epoch 28 | Loss: 0.089653 | Probs: [0.04402322 0.9142488  0.04172796]\n",
      "Epoch 29 | Loss: 0.085031 | Probs: [0.04186038 0.9184836  0.03965606]\n",
      "Epoch 30 | Loss: 0.080797 | Probs: [0.03986866 0.9223812  0.0377501 ]\n",
      "Epoch 31 | Loss: 0.076906 | Probs: [0.03803018 0.9259773  0.0359925 ]\n",
      "Epoch 32 | Loss: 0.073321 | Probs: [0.03632937 0.9293027  0.03436796]\n",
      "Epoch 33 | Loss: 0.070010 | Probs: [0.03475253 0.9323845  0.03286307]\n",
      "Epoch 34 | Loss: 0.066945 | Probs: [0.0332876  0.93524635 0.03146608]\n",
      "Epoch 35 | Loss: 0.064102 | Probs: [0.031924   0.93790936 0.03016666]\n",
      "Epoch 36 | Loss: 0.061458 | Probs: [0.03065234 0.940392   0.02895567]\n",
      "Epoch 37 | Loss: 0.058996 | Probs: [0.02946429 0.94271064 0.02782501]\n",
      "Epoch 38 | Loss: 0.056697 | Probs: [0.02835247 0.94488    0.02676752]\n",
      "Epoch 39 | Loss: 0.054548 | Probs: [0.0273103  0.94691294 0.02577682]\n",
      "Epoch 40 | Loss: 0.052535 | Probs: [0.02633186 0.948821   0.02484719]\n",
      "Epoch 41 | Loss: 0.050647 | Probs: [0.02541189 0.9506146  0.02397354]\n",
      "Epoch 42 | Loss: 0.048872 | Probs: [0.02454565 0.95230305 0.0231513 ]\n",
      "Epoch 43 | Loss: 0.047202 | Probs: [0.02372888 0.9538948  0.02237637]\n",
      "Epoch 44 | Loss: 0.045628 | Probs: [0.02295775 0.9553973  0.02164502]\n",
      "Epoch 45 | Loss: 0.044143 | Probs: [0.02222878 0.9568173  0.02095394]\n",
      "Epoch 46 | Loss: 0.042739 | Probs: [0.02153881 0.9581612  0.02030007]\n",
      "Epoch 47 | Loss: 0.041411 | Probs: [0.02088501 0.9594343  0.01968069]\n",
      "Epoch 48 | Loss: 0.040153 | Probs: [0.02026477 0.960642   0.01909331]\n",
      "Epoch 49 | Loss: 0.038961 | Probs: [0.01967574 0.96178854 0.01853566]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0197, 0.9618, 0.0185]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_dim = 4\n",
    "output_dim = 3\n",
    "lr = 0.1\n",
    "\n",
    "vocab_size = 5\n",
    "embed_dim = 4  \n",
    "\n",
    "E = torch.randn(vocab_size, embed_dim, requires_grad=False) #[5 , 4]\n",
    "\n",
    "input_ids = torch.tensor([1, 3, 4])  # [3] \n",
    "\n",
    "y_true = torch.tensor([1])  # [1]\n",
    "\n",
    "\n",
    "weights = torch.randn(embed_dim, output_dim, requires_grad=False)  # [4, 3]\n",
    "bias = torch.zeros(output_dim)  # [3]\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    # embed\n",
    "    x = E[input_ids] #  [3 , 4]\n",
    "    \n",
    "    #avg to handle easily\n",
    "    x_ = x.mean(dim = 0 , keepdim = True)  #[1 , 4]\n",
    "\n",
    "    #weights  \n",
    "    z = torch.matmul(x_ , weights) + bias #[1 , 3] + [3] -> [1 , 3]\n",
    "\n",
    "    z_ = torch.exp(z - torch.max(z))\n",
    "    sum_ = torch.sum(z_ , dim = -1 , keepdim = True) #[1 , 1]\n",
    "\n",
    "    S = z_ / sum_ #[1 , 3]\n",
    "\n",
    "    loss = -torch.log(S[0 , y_true])\n",
    "\n",
    "    y_hot = torch.zeros_like(S)\n",
    "    y_hot[0 , y_true] = 1.0\n",
    "\n",
    "    dz = S - y_hot #[1 , 3]\n",
    "\n",
    "    dw = torch.matmul(x_.T , dz) #[4 , 3]\n",
    "    db = dz.sum(dim = 0)\n",
    "\n",
    "    dx_ = torch.matmul(dz , weights.T)\n",
    "    \n",
    "\n",
    "    dE = torch.zeros_like(E)\n",
    "\n",
    "    for idx in input_ids:\n",
    "        dE[idx] += dx_.squeeze(0)\n",
    "\n",
    "    weights -= lr * dw\n",
    "    bias -= lr * db\n",
    "    E -= lr * dE\n",
    " \n",
    "    print(f\"Epoch {epoch:2d} | Loss: {loss.item():.6f} | Probs: {S[0].detach().numpy()}\")\n",
    "\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0754fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
