{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcdd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self , embed_dim , hid_dim , dropout):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Parameter(torch.randn(hid_dim , embed_dim)) #(hid , embed)\n",
    "        self.l1bias = nn.Parameter(torch.randn(hid_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear2 = nn.Parameter(torch.randn(embed_dim , hid_dim)) #(embed , hid)\n",
    "        self.l2bias = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.linear1)\n",
    "        nn.init.xavier_uniform_(self.linear2)\n",
    "        nn.init.zeros_(self.l1bias)\n",
    "        nn.init.zeros_(self.l2bias)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "    \n",
    "    def forward(self , x): #x -> (batch , seq , embed) \n",
    "        # batch , seq , hid\n",
    "        hidden = self.act(x @ self.linear1.T + self.l1bias) #(batch , seq , embed) @ (embed , hid) -> (batch , seq , hid)\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = hidden @ self.linear2.T + self.l2bias #(batch , seq , hid) @ \n",
    "        return  output \n",
    "    # we project to higher dims to have more features to be learned. \n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self , features , eps = 10 **-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self , x):\n",
    "        mean = x.mean(dim = -1 , keepdim = True)\n",
    "        std  = x.std(dim = -1 , keepdim  = True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self , features , dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x , sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x))) # sublayer is layer we skip while doing connection\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size , embed_dim) # row , columns   \n",
    "\n",
    "    def forward(self , x):\n",
    "        out = self.embed(x) * math.sqrt(self.embed_dim)\n",
    "        return  out\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self , embed_dim , seq_len , dropout : float):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        position = torch.arange(0 , seq_len , dtype = torch.float).unsqueeze(1) \n",
    "        pe = torch.zeros(seq_len, embed_dim)\n",
    "\n",
    "        # (sin(position / 10000 ** (2i / embed-dim))) lets call 10000 ** (2i / embed-dim)) term\n",
    "        term = torch.exp(torch.arange(0 , embed_dim , 2).float() * -math.log(10000.0) / embed_dim)\n",
    "        pe[: , 0::2] = torch.sin(position * term)\n",
    "        pe[: , 1::2] = torch.cos(position * term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self , z):\n",
    "        # z -> (batch , seq_len , embed_dim)\n",
    "        return self.dropout(z + (self.pe[: , :z.shape[1] , :]).requires_grad_(False))\n",
    "\n",
    "def RoPE(x, sin, cos, start_pos=0):\n",
    "    seq = x.size(-2)\n",
    "    head_dim = x.size(-1)\n",
    "    num_head_x = x.size(1)  # This could be 8 (query) or 1 (key)\n",
    "    \n",
    "    embed_dim = sin.size(-1)\n",
    "    total_heads = embed_dim // head_dim  # This should be 8\n",
    "    \n",
    "    sin_sliced = sin[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    cos_sliced = cos[start_pos : start_pos + seq]  # [seq, embed_dim]\n",
    "    \n",
    "    sin_sliced = sin_sliced.view(seq, total_heads, head_dim)\n",
    "    cos_sliced = cos_sliced.view(seq, total_heads, head_dim)\n",
    "    \n",
    "    if total_heads != num_head_x:\n",
    "        sin_sliced = sin_sliced[:, :num_head_x, :]\n",
    "        cos_sliced = cos_sliced[:, :num_head_x, :]\n",
    "    \n",
    "    sin_sliced = sin_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    cos_sliced = cos_sliced.permute(1, 0, 2).unsqueeze(0)  # [1, num_head_x, seq, head_dim]\n",
    "    \n",
    "   \n",
    "    x_even = x[..., 0::2]\n",
    "    x_odd = x[..., 1::2]\n",
    "    \n",
    "    sin_even = sin_sliced[..., 0::2]\n",
    "    sin_odd = sin_sliced[..., 1::2]\n",
    "    cos_even = cos_sliced[..., 0::2]\n",
    "    cos_odd = cos_sliced[..., 1::2]\n",
    "    \n",
    "    rotated = torch.empty_like(x)\n",
    "    rotated[..., 0::2] = x_even * cos_even - x_odd * sin_even\n",
    "    rotated[..., 1::2] = x_even * sin_odd + x_odd * cos_odd\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "def RoPE_Embed(dim , seq):\n",
    "    \n",
    "    term = 1.0 / (10000 ** (torch.arange(0 , dim , 2).float() / dim))\n",
    "    seq_ = torch.arange(seq).float()\n",
    "\n",
    "    emb = torch.outer(seq_ , term)\n",
    "    emb = torch.cat([emb , emb] , dim = -1)\n",
    "\n",
    "    return emb.sin() , emb.cos()\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch , math\n",
    "\n",
    "def Rope_embed(dim , seq):\n",
    "\n",
    "    term = 1.0 / (10000 ** (torch.arange(0 , dim , 2).float() / dim))\n",
    "    seq_ = torch.arange(seq).float()\n",
    "    emb = torch.outer(seq_ , term)\n",
    "    # emb = torch.cat([emb , emb] , dim = -1)\n",
    "    emb = torch.stack([emb, emb], dim=-1).flatten(-2)\n",
    "\n",
    "    return emb.sin() , emb.cos()\n",
    "\n",
    "\n",
    "def RoPE(emb, sin, cos, start_pos=0):\n",
    "    # emb shape: (Batch, Heads, Seq, Head_Dim)\n",
    "    seq_len = emb.size(-2)\n",
    "    head_dim = emb.size(-1)\n",
    "    \n",
    "\n",
    "    s = sin[start_pos : start_pos + seq_len, :head_dim]\n",
    "    c = cos[start_pos : start_pos + seq_len, :head_dim]\n",
    "    \n",
    "    s = s.unsqueeze(0).unsqueeze(0)\n",
    "    c = c.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    x1 = emb[..., 0::2]\n",
    "    x2 = emb[..., 1::2]\n",
    "    \n",
    "    res = torch.empty_like(emb)\n",
    "    res[..., 0::2] = x1 * c[..., 0::2] - x2 * s[..., 0::2]\n",
    "    res[..., 1::2] = x1 * s[..., 1::2] + x2 * c[..., 1::2]\n",
    "    return res\n",
    "\n",
    "def RoPE_backward(grad_output, sin, cos, start_pos=0):\n",
    "    \n",
    "    seq_len = grad_output.size(-2)\n",
    "    head_dim = grad_output.size(-1)\n",
    "\n",
    "    s = sin[start_pos : start_pos + seq_len, :head_dim].unsqueeze(0).unsqueeze(0)\n",
    "    c = cos[start_pos : start_pos + seq_len, :head_dim].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    gy1 = grad_output[..., 0::2]\n",
    "    gy2 = grad_output[..., 1::2]\n",
    "\n",
    "    grad_input = torch.empty_like(grad_output)\n",
    "    \n",
    "    grad_input[..., 0::2] =  gy1 * c[..., 0::2] + gy2 * s[..., 1::2]\n",
    "    grad_input[..., 1::2] = -gy1 * s[..., 0::2] + gy2 * c[..., 1::2]\n",
    "    return grad_input\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self , embed_dim , numhead , start_pos = 0):\n",
    "        super().__init__()\n",
    "        self.start = start_pos\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_head = numhead\n",
    "\n",
    "        assert (embed_dim % numhead == 0) , \"try a different number\"\n",
    "\n",
    "        self.d_k = embed_dim // numhead\n",
    "\n",
    "        self.wq = nn.Parameter(torch.randn(embed_dim , embed_dim))\n",
    "        self.wk = nn.Parameter(torch.randn(embed_dim , embed_dim))\n",
    "        self.wv = nn.Parameter(torch.randn(embed_dim , embed_dim))\n",
    "\n",
    "        self.wo = nn.Parameter(torch.randn(embed_dim , embed_dim))\n",
    "\n",
    "        self.qb = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.kb = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.vb = nn.Parameter(torch.randn(embed_dim))\n",
    "\n",
    "        for name in [self.wq , self.wk , self.wv]:\n",
    "            nn.init.kaiming_uniform_(name)\n",
    "\n",
    "        for name in [self.qb , self.kb , self.vb]:\n",
    "            nn.init.zeros_(name)\n",
    "\n",
    "        self.register_buffer('sin', None)\n",
    "        self.register_buffer('cos', None)\n",
    "    \n",
    "    def precompute_emb(self , seq):\n",
    "        if self.sin is None or self.sin.size(0) <= seq:\n",
    "            sin , cos = Rope_embed(dim = self.embed_dim , seq = seq)\n",
    "            self.register_buffer('sin' , sin)\n",
    "            self.register_buffer('cos' , cos)\n",
    "\n",
    "    def forward(self , q , k , v , mask = None , pastlayer = None):\n",
    "\n",
    "        # shapes of all q , k , v = (batch , seq , embed)\n",
    "        self.batch = q.size(0)\n",
    "        self.seq = q.size(1)\n",
    "\n",
    "        self.query = q\n",
    "        self.key = k\n",
    "        self.value = v\n",
    "\n",
    "        q = q @ self.wq + self.qb\n",
    "        k = k @ self.wk + self.kb\n",
    "        v = v @ self.wv + self.vb\n",
    "\n",
    "        # if pastlayer is not None:\n",
    "        #     key_ , value_ = pastlayer\n",
    "        #     k = torch.cat([key_ , k])\n",
    "        #     v = torch.cat([value_ , v])\n",
    "        \n",
    "        \n",
    "        present = (k , v)\n",
    "\n",
    "    \n",
    "        q_split = q.view(self.batch, self.seq, self.num_head, self.d_k)\n",
    "        k_split = k.view(self.batch, self.seq, self.num_head, self.d_k)\n",
    "        v_split = v.view(self.batch, self.seq, self.num_head, self.d_k)\n",
    "\n",
    "        #  Permute to bring num_heads to the second dimension\n",
    "        # shape: (batch, num_heads, seq, d_k)\n",
    "        self.query_reshape = q_split.transpose(1, 2)\n",
    "        self.key_reshape   = k_split.transpose(1, 2)\n",
    "        self.value_reshaped = v_split.transpose(1, 2)\n",
    "        \n",
    "\n",
    "        total_seq = max(q.size(1) + self.start , k.size(1) , (pastlayer[0].size(2) if pastlayer else 0))\n",
    "        self.precompute_emb(total_seq)\n",
    "        self.query_reshaped = RoPE(self.query_reshape , self.sin , self.cos , self.start)\n",
    "        self.key_reshaped = RoPE(self.key_reshape , self.sin , self.cos , 0)\n",
    "\n",
    "        self.score_bf_sf = torch.matmul(self.query_reshaped , self.key_reshaped.transpose(-2 , -1)) / math.sqrt(self.d_k) \n",
    "\n",
    "\n",
    "        if mask is not None:\n",
    "            current_k = self.key_reshaped.shape[-2]\n",
    "            current_q = self.query_reshaped.shape[-2]\n",
    "            if pastlayer is not None:\n",
    "                casual_mask = torch.tril(torch.ones(current_k , current_k))\n",
    "                casual_mask_shaped = casual_mask[-current_q , :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "                if mask.size(0) >= current_k:\n",
    "                    padding_mask = mask[... , -current_k:]\n",
    "                else:\n",
    "                    padding_mask = torch.ones_like(mask[..., :current_k])\n",
    "                \n",
    "                casual_mask_shaped = casual_mask_shaped.expand(mask.size(0) , -1 , -1 , -1)\n",
    "                combinedmask = casual_mask_shaped * padding_mask\n",
    "\n",
    "            else:\n",
    "                if mask.size(0) != current_k:\n",
    "                    mask = mask[... , -current_k:]\n",
    "                elif mask.dim() == 3:\n",
    "                    mask = mask.unsqueeze(0)\n",
    "                \n",
    "                combinedmask = mask\n",
    "            self.combined_mask = combinedmask\n",
    "            self.score_bf_sf = self.score_bf_sf.masked_fill_(combinedmask == 0 , -1e9)\n",
    "\n",
    "        max_scores = torch.max(self.score_bf_sf , dim = -1 , keepdim = True)[0]\n",
    "        exp_scores = torch.exp(self.score_bf_sf - max_scores)\n",
    "        sum_scores = torch.sum(exp_scores , dim = -1 , keepdim = True)\n",
    "\n",
    "        self.scores_sf = exp_scores / sum_scores\n",
    "\n",
    "        out = self.scores_sf @ self.value_reshaped\n",
    "        # out shape (batch , num_head , seq , d_k)\n",
    "\n",
    "        self.out = out.permute(0 , 2 , 1 , 3).contiguous().view(self.batch , self.seq , -1) #(batch , seq , embed)\n",
    "    \n",
    "\n",
    "        self.context_merged = self.out\n",
    "        return self.context_merged @ self.wo , present\n",
    "\n",
    "    \n",
    "    def backward(self , dout):\n",
    "        batch , seq , embed = dout.shape\n",
    "\n",
    "    #     #(shape of dout = batch , seq , embed_dim) , (shape of self.out = batch , seq , embed)\n",
    "        self.dwo = torch.einsum(\"bsi,bsj->ij\", self.context_merged, dout)   #(batch , embed , embed)\n",
    "    #     # dwo = dwo.sum(dim = 0) #(to get real weight projection across all the batches -> embed - embed)\n",
    "\n",
    "    #     # we need to extract the weights from the current output because we will backprop on current condition\n",
    "        d_context_merged = dout @ self.wo.transpose(-2 , -1) # this is the weight of current (you can say actual attn weights with softmax)\n",
    "    #     # it has a shape of (batch , seq , embed)\n",
    "\n",
    "\n",
    "    #     # now we need it to be in 4D (as our 'out' is in 4d when we computed across values)\n",
    "        d_context = d_context_merged.view(batch , seq , self.num_head , self.d_k).permute(0 , 2 , 1 , 3)\n",
    "    #     #d_context shape = batch , numhead , seq , d_k\n",
    "\n",
    "        \n",
    "    #     # now we can go for value and actualy softmax scores\n",
    "\n",
    "    #     # self.score_sf shape = (batch , numhead , seq , seq)\n",
    "        dv = self.scores_sf.transpose(-2, -1) @ d_context\n",
    "\n",
    "    #     #dv shape (batch , numhead , seq , d_k)\n",
    "\n",
    "    #     # we will use real reshaped value\n",
    "        d_score = torch.matmul(d_context, self.value_reshaped.transpose(-2, -1))\n",
    "        \n",
    "        if hasattr(self, 'combined_mask') and self.combined_mask is not None:\n",
    "            d_score = d_score.masked_fill(self.combined_mask == 0, 0.0)\n",
    "\n",
    "\n",
    "        # now we have to backtrack to get non softmax scores \n",
    "        \"\"\"\n",
    "             so here we have to be careful , like the derivative is S(1 - S) 'i = j' and -SS ' i != j'  \n",
    "             it can be written as diag(S) - SS as it follow this \n",
    "             and we differentiate wrt to i and j \n",
    "             we'd get Sj * (dSj - submission(Si * dSj))\n",
    "\n",
    "             where dSj is the change in scores , submission\n",
    "         \"\"\"\n",
    "\n",
    "        sum_dp = torch.sum(d_score * self.scores_sf , dim = -1 , keepdim = True)\n",
    "        before_sf_score = (self.scores_sf * (d_score - sum_dp)) / math.sqrt(self.d_k)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #shape for before_sf_score would be (batch , numhead , seq , seq)\n",
    "\n",
    "        # now we have the non softmax scores now we have to backprop to get self.wq and all other and actual query and key , value proj \n",
    "        # to get their we need to get all dq , dk , dv , for this we need actual query and all \n",
    "\n",
    "        dq = torch.matmul(before_sf_score, self.key_reshaped)\n",
    "        dk = torch.matmul(\n",
    "            before_sf_score.transpose(-2, -1),\n",
    "            self.query_reshaped\n",
    "         )\n",
    "\n",
    "\n",
    "      # dq_unrotated = RoPE(dq , -self.sin , self.cos , self.start)\n",
    "       # dk_unrotated = RoPE(dk , -self.sin , self.cos , 0)\n",
    "\n",
    "        dq_unrotated = RoPE_backward(dq , self.sin , self.cos , self.start)\n",
    "        dk_unrotated = RoPE_backward(dk , self.sin , self.cos , 0)\n",
    "       # dq_unrotated = dq\n",
    "        # dk_unrotated = dk\n",
    "\n",
    "        q_reshaped = dq_unrotated.transpose(2 , 1).contiguous().view(batch , seq , -1)\n",
    "        k_reshaped = dk_unrotated.transpose(2 , 1).contiguous().view(batch , seq , -1)\n",
    "        v_reshaped = dv.transpose(2 , 1).contiguous().view(batch , seq , -1)\n",
    "\n",
    "\n",
    "        # here is little confusing because query = q @ self.wq , do we need to be careful while using the real and reshaped grad query \n",
    "        # and another thing we have to make (embed , embed) so we gonna flat it \n",
    "        real_q = self.query.view(-1 , embed)\n",
    "        grad_q = q_reshaped.view(-1 , embed)\n",
    "        self.dwq = (real_q.T @ grad_q)\n",
    "        self.dqb = q_reshaped.sum(dim = (0 , 1))\n",
    "\n",
    "        # same for all other \n",
    "        real_k = self.key.view(-1 , embed)\n",
    "        grad_k = k_reshaped.view(-1 , embed)\n",
    "        self.dwk = (real_k.T @ grad_k)\n",
    "        self.dkb = k_reshaped.sum(dim = (0 , 1))\n",
    "\n",
    "        real_v = self.value.view(-1 , embed)\n",
    "        grad_v = v_reshaped.view(-1 , embed)\n",
    "        self.dwv = (real_v.T @ grad_v)\n",
    "        self.dvb = v_reshaped.sum(dim = (0 , 1))\n",
    "\n",
    "        dx_q = q_reshaped @ self.wq.T\n",
    "        dx_k = k_reshaped @ self.wk.T\n",
    "        dx_v = v_reshaped @ self.wv.T\n",
    "        return dx_q + dx_k + dx_v\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self , attention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = attention\n",
    "        self.ffn = ffn\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "\n",
    "        \n",
    "    def forward(self , x , mask):\n",
    "        \n",
    "        selfattn , _ = self.attention(x , x , x , mask)\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda x: self.ffn(x))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(features)\n",
    "        self.layer = layers\n",
    "\n",
    "    def forward(self , x , mask):\n",
    "        for layer in self.layer:\n",
    "            x = layer(x , mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self , selfAttention , crossAttention , ffn , feat , dropout):\n",
    "        super().__init__()\n",
    "        self.attention = selfAttention\n",
    "        self.ffn = ffn\n",
    "        self.cross = crossAttention\n",
    "        self.residual1 = Residual(feat , dropout)\n",
    "        self.residual2 = Residual(feat , dropout)\n",
    "        self.residual3 = Residual(feat , dropout)\n",
    "\n",
    "    def forward(self , x , enoderOutput , selfmask , crossmask , pastlayergroup):\n",
    "        selfpast , _ = pastlayergroup if pastlayergroup is not None else (None , None)\n",
    "        \n",
    "        selfattn , selfpresent = self.attention(x , x , x , selfmask , pastlayer = selfpast)\n",
    "        crossattn , crosspresent = self.cross(x , enoderOutput , enoderOutput , crossmask , pastlayer = None)\n",
    "\n",
    "        x = self.residual1(x , lambda _: selfattn)\n",
    "        x = self.residual2(x , lambda _: crossattn)\n",
    "        x = self.residual3(x , lambda x: self.ffn(x))\n",
    "        return x , (selfpresent , None)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , features , layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self , x , enc_out , tgt_mask , src_mask , pastvalues):\n",
    "        new = []\n",
    "        for i , layer in enumerate(self.layers):\n",
    "            past = pastvalues[i] if pastvalues else None\n",
    "            x , layergrp = layer(x , enc_out , tgt_mask , src_mask , pastlayergroup = past)\n",
    "            new.append(layergrp)\n",
    "\n",
    "        return self.norm(x) , new\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self , embed_dim , vocab_size):\n",
    "        super().__init__()\n",
    "        self.project = nn.Parameter(torch.randn(vocab_size , embed_dim))\n",
    "        self.bias = nn.Parameter(torch.randn(vocab_size))\n",
    "        nn.init.xavier_uniform_(self.project)\n",
    "        nn.init.zeros_(self.bias)\n",
    "        # self.project = nn.Linear(embed_dim , vocab_size)\n",
    "\n",
    "    def forward(self , x): #(batch , seq , embed)\n",
    "        return x @ self.project.T + self.bias\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self , encoder : Encoder , decoder : Decoder , src_emb , tgt_emb , src_pos , tgt_pos , proj_layer):\n",
    "        super().__init__()\n",
    "        self.encoder_ = encoder\n",
    "        self.decoder_ = decoder\n",
    "        self.src_emb = src_emb\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.proj_layer = proj_layer \n",
    "    \n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder_(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_out, tgt_mask, src_mask, pastvalues=None):\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder_(tgt, encoder_out, tgt_mask, src_mask, pastvalues)\n",
    "    \n",
    "    def projection(self, x):\n",
    "        return self.proj_layer(x)\n",
    "\n",
    "def build(src_vocab , tgt_vocab , src_seq , tgt_seq , embed_dim = 512 , num_head = 8 , num_layer = 6 , dropout =  float(0.1) , hid_dim = 2048):\n",
    "    src_embed = Embedding(embed_dim = embed_dim , vocab_size = src_vocab)\n",
    "    tgt_embed = Embedding(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    src_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = src_seq , dropout = dropout)\n",
    "    tgt_pos = PositionalEmbedding(embed_dim = embed_dim , seq_len = tgt_seq , dropout = dropout)\n",
    "\n",
    "    enc_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        enc_attn = MultiHeadAttention(embed_dim , num_head , dropout )\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        enc_block = EncoderBlock(attention = enc_attn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        enc_blocks.append(enc_block)\n",
    "\n",
    "    dec_blocks = []\n",
    "\n",
    "    for _ in range(num_layer):\n",
    "        dec_selfAttn = MultiHeadAttention(embed_dim , num_head , dropout )\n",
    "        dec_crossAttn = MultiHeadAttention(embed_dim , num_head , dropout)\n",
    "        ffn = FFN(embed_dim , hid_dim , dropout)\n",
    "        dec_block = DecoderBlock(selfAttention = dec_selfAttn , crossAttention = dec_crossAttn , ffn = ffn , feat = embed_dim , dropout = dropout)\n",
    "        dec_blocks.append(dec_block)\n",
    "\n",
    "    encoder = Encoder(features = embed_dim , layers = nn.ModuleList(enc_blocks))\n",
    "    decoder = Decoder(features = embed_dim , layers = nn.ModuleList(dec_blocks))\n",
    "\n",
    "    proj_layer = Projection(embed_dim = embed_dim , vocab_size = tgt_vocab)\n",
    "\n",
    "    transformer = Transformer(encoder = encoder , decoder = decoder , src_emb = src_embed , tgt_emb = tgt_embed , src_pos = src_pos , tgt_pos = tgt_pos , proj_layer = proj_layer)\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer\n",
    "        \n",
    "transformer = build(src_seq = 8 , tgt_seq = 8 , src_vocab = 50 , tgt_vocab = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e0f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
