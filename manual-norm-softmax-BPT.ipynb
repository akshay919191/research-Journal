{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66db8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch , math\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "q = torch.randn(1 , 8 , 8 , 64) #(batch , numHEAD , seq_len , head_dim)\n",
    "k = torch.randn(1 , 8 , 8 , 64)\n",
    "v = torch.randn(1 , 8 , 8 , 64)\n",
    "\n",
    "q.requires_grad_(True)\n",
    "k.requires_grad_(True)\n",
    "v.requires_grad_(True)\n",
    "\n",
    "\n",
    "y_true = torch.randint(0, 64, (1, 8))\n",
    "\n",
    "# y = wx + b  , where x is the data\n",
    "for epoch in range(50):\n",
    "    # z = torch.matmul(dummy , weights) + bias\n",
    "    scores = torch.matmul(q , k.transpose(-2 , -1)) / math.sqrt(k.shape[-1])\n",
    "    \n",
    "    max_ = torch.max(scores , dim = -1 , keepdim = True)[0]\n",
    "    exp_z = torch.exp(scores - max_) \n",
    "    sum_ = torch.sum(exp_z, dim=-1, keepdim=True)\n",
    " \n",
    "    P = exp_z / sum_\n",
    "\n",
    "    S = torch.matmul(P , v)\n",
    "\n",
    "    S_mean = S.mean(dim=1)  \n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(S_mean, dim=-1)\n",
    "    loss = -log_probs[torch.arange(1).unsqueeze(1), torch.arange(8), y_true].mean()\n",
    "\n",
    "    ds = torch.nn.functional.softmax(S_mean , dim = 1)\n",
    "    ds[torch.arange(1).unsqueeze(1) , torch.arange(8) , y_true] -= 1\n",
    "    ds = ds / (1 * 8)\n",
    "\n",
    "    num_heads = 8\n",
    "    ds_expanded = ds.unsqueeze(1).expand(-1, num_heads, -1, -1) / num_heads\n",
    "\n",
    "    dv = torch.matmul(P.transpose(-2, -1), ds_expanded) # [1, 8, 8, 64]\n",
    "    dp = torch.matmul(ds_expanded, v.transpose(-2, -1)) # [1, 8, 8, 8]\n",
    "\n",
    "    d_scores = P * (dp - torch.sum(P * dp, dim=-1, keepdim=True))\n",
    "\n",
    "    scale = 1.0 / math.sqrt(k.shape[-1])\n",
    "\n",
    "    dq = torch.matmul(d_scores * scale , k)\n",
    "\n",
    "    dk = torch.matmul(d_scores.transpose(-2 , -1) * scale , q)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q -= lr * dq\n",
    "        k -= lr * dk\n",
    "        v -= lr * dv\n",
    "\n",
    "    print(f\"loss for epoch {epoch} : {loss.item()}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d5cf649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss over epoch 1 : 4.164918899536133\n",
      "loss over epoch 2 : 4.163978576660156\n",
      "loss over epoch 3 : 4.162956237792969\n",
      "loss over epoch 4 : 4.16184663772583\n",
      "loss over epoch 5 : 4.160642623901367\n",
      "loss over epoch 6 : 4.159340858459473\n",
      "loss over epoch 7 : 4.157934665679932\n",
      "loss over epoch 8 : 4.156418800354004\n",
      "loss over epoch 9 : 4.154788970947266\n",
      "loss over epoch 10 : 4.153040885925293\n",
      "loss over epoch 11 : 4.151169776916504\n",
      "loss over epoch 12 : 4.149171829223633\n",
      "loss over epoch 13 : 4.1470465660095215\n",
      "loss over epoch 14 : 4.1447906494140625\n",
      "loss over epoch 15 : 4.142405033111572\n",
      "loss over epoch 16 : 4.139889717102051\n",
      "loss over epoch 17 : 4.137246131896973\n",
      "loss over epoch 18 : 4.134479522705078\n",
      "loss over epoch 19 : 4.131594181060791\n",
      "loss over epoch 20 : 4.128596305847168\n",
      "loss over epoch 21 : 4.125494003295898\n",
      "loss over epoch 22 : 4.122298240661621\n",
      "loss over epoch 23 : 4.119019508361816\n",
      "loss over epoch 24 : 4.115668773651123\n",
      "loss over epoch 25 : 4.112261772155762\n",
      "loss over epoch 26 : 4.108811378479004\n",
      "loss over epoch 27 : 4.105332851409912\n",
      "loss over epoch 28 : 4.101842403411865\n",
      "loss over epoch 29 : 4.098356246948242\n",
      "loss over epoch 30 : 4.094889163970947\n",
      "loss over epoch 31 : 4.091456413269043\n",
      "loss over epoch 32 : 4.08807373046875\n",
      "loss over epoch 33 : 4.084753513336182\n",
      "loss over epoch 34 : 4.081510543823242\n",
      "loss over epoch 35 : 4.078354835510254\n",
      "loss over epoch 36 : 4.0752973556518555\n",
      "loss over epoch 37 : 4.072347164154053\n",
      "loss over epoch 38 : 4.069510459899902\n",
      "loss over epoch 39 : 4.066794395446777\n",
      "loss over epoch 40 : 4.064202785491943\n",
      "loss over epoch 41 : 4.061737537384033\n",
      "loss over epoch 42 : 4.059401512145996\n",
      "loss over epoch 43 : 4.05719518661499\n",
      "loss over epoch 44 : 4.055116176605225\n",
      "loss over epoch 45 : 4.053164005279541\n",
      "loss over epoch 46 : 4.051333904266357\n",
      "loss over epoch 47 : 4.049624919891357\n",
      "loss over epoch 48 : 4.048030376434326\n",
      "loss over epoch 49 : 4.0465474128723145\n",
      "loss over epoch 50 : 4.045170307159424\n",
      "loss over epoch 51 : 4.043894290924072\n",
      "loss over epoch 52 : 4.042713165283203\n",
      "loss over epoch 53 : 4.041621208190918\n",
      "loss over epoch 54 : 4.040615081787109\n",
      "loss over epoch 55 : 4.039687156677246\n",
      "loss over epoch 56 : 4.038832664489746\n",
      "loss over epoch 57 : 4.038046836853027\n",
      "loss over epoch 58 : 4.037324905395508\n",
      "loss over epoch 59 : 4.036661624908447\n",
      "loss over epoch 60 : 4.036052703857422\n",
      "loss over epoch 61 : 4.035493850708008\n",
      "loss over epoch 62 : 4.0349812507629395\n",
      "loss over epoch 63 : 4.034511089324951\n",
      "loss over epoch 64 : 4.034079551696777\n",
      "loss over epoch 65 : 4.033684253692627\n",
      "loss over epoch 66 : 4.033321380615234\n",
      "loss over epoch 67 : 4.03298807144165\n",
      "loss over epoch 68 : 4.032682418823242\n",
      "loss over epoch 69 : 4.0324015617370605\n",
      "loss over epoch 70 : 4.032142639160156\n",
      "loss over epoch 71 : 4.031905174255371\n",
      "loss over epoch 72 : 4.031686305999756\n",
      "loss over epoch 73 : 4.031484603881836\n",
      "loss over epoch 74 : 4.031298637390137\n",
      "loss over epoch 75 : 4.031126022338867\n",
      "loss over epoch 76 : 4.0309672355651855\n",
      "loss over epoch 77 : 4.030819416046143\n",
      "loss over epoch 78 : 4.030682563781738\n",
      "loss over epoch 79 : 4.03055477142334\n",
      "loss over epoch 80 : 4.0304365158081055\n",
      "loss over epoch 81 : 4.030325889587402\n",
      "loss over epoch 82 : 4.030222415924072\n",
      "loss over epoch 83 : 4.030125617980957\n",
      "loss over epoch 84 : 4.03003454208374\n",
      "loss over epoch 85 : 4.029948711395264\n",
      "loss over epoch 86 : 4.029867649078369\n",
      "loss over epoch 87 : 4.029791355133057\n",
      "loss over epoch 88 : 4.02971887588501\n",
      "loss over epoch 89 : 4.029650688171387\n",
      "loss over epoch 90 : 4.029585361480713\n",
      "loss over epoch 91 : 4.029522895812988\n",
      "loss over epoch 92 : 4.029462814331055\n",
      "loss over epoch 93 : 4.0294060707092285\n",
      "loss over epoch 94 : 4.029350757598877\n",
      "loss over epoch 95 : 4.029297828674316\n",
      "loss over epoch 96 : 4.029247283935547\n",
      "loss over epoch 97 : 4.029197692871094\n",
      "loss over epoch 98 : 4.029150009155273\n",
      "loss over epoch 99 : 4.0291032791137695\n",
      "loss over epoch 100 : 4.029058456420898\n",
      "loss over epoch 101 : 4.029015064239502\n",
      "loss over epoch 102 : 4.0289716720581055\n",
      "loss over epoch 103 : 4.028930187225342\n",
      "loss over epoch 104 : 4.028889179229736\n",
      "loss over epoch 105 : 4.028849124908447\n",
      "loss over epoch 106 : 4.028810024261475\n",
      "loss over epoch 107 : 4.02877140045166\n",
      "loss over epoch 108 : 4.028733253479004\n",
      "loss over epoch 109 : 4.028696060180664\n",
      "loss over epoch 110 : 4.028659343719482\n",
      "loss over epoch 111 : 4.028623580932617\n",
      "loss over epoch 112 : 4.028587341308594\n",
      "loss over epoch 113 : 4.028552055358887\n",
      "loss over epoch 114 : 4.028517246246338\n",
      "loss over epoch 115 : 4.028482913970947\n",
      "loss over epoch 116 : 4.028448581695557\n",
      "loss over epoch 117 : 4.028414726257324\n",
      "loss over epoch 118 : 4.02838134765625\n",
      "loss over epoch 119 : 4.028347969055176\n",
      "loss over epoch 120 : 4.02831506729126\n",
      "loss over epoch 121 : 4.028282642364502\n",
      "loss over epoch 122 : 4.028250217437744\n",
      "loss over epoch 123 : 4.0282182693481445\n",
      "loss over epoch 124 : 4.028186321258545\n",
      "loss over epoch 125 : 4.0281548500061035\n",
      "loss over epoch 126 : 4.028123378753662\n",
      "loss over epoch 127 : 4.028092384338379\n",
      "loss over epoch 128 : 4.028061389923096\n",
      "loss over epoch 129 : 4.0280303955078125\n",
      "loss over epoch 130 : 4.028000354766846\n",
      "loss over epoch 131 : 4.027969837188721\n",
      "loss over epoch 132 : 4.027939796447754\n",
      "loss over epoch 133 : 4.027909278869629\n",
      "loss over epoch 134 : 4.02787971496582\n",
      "loss over epoch 135 : 4.027850151062012\n",
      "loss over epoch 136 : 4.027821063995361\n",
      "loss over epoch 137 : 4.027791500091553\n",
      "loss over epoch 138 : 4.027761936187744\n",
      "loss over epoch 139 : 4.027733325958252\n",
      "loss over epoch 140 : 4.02770471572876\n",
      "loss over epoch 141 : 4.027676105499268\n",
      "loss over epoch 142 : 4.027647018432617\n",
      "loss over epoch 143 : 4.027619361877441\n",
      "loss over epoch 144 : 4.027591228485107\n",
      "loss over epoch 145 : 4.027562618255615\n",
      "loss over epoch 146 : 4.027535438537598\n",
      "loss over epoch 147 : 4.027507305145264\n",
      "loss over epoch 148 : 4.027480125427246\n",
      "loss over epoch 149 : 4.02745246887207\n",
      "loss over epoch 150 : 4.027425289154053\n",
      "loss over epoch 151 : 4.027398109436035\n",
      "loss over epoch 152 : 4.027370929718018\n",
      "loss over epoch 153 : 4.027344703674316\n",
      "loss over epoch 154 : 4.027318000793457\n",
      "loss over epoch 155 : 4.027291297912598\n",
      "loss over epoch 156 : 4.027264595031738\n",
      "loss over epoch 157 : 4.027238845825195\n",
      "loss over epoch 158 : 4.027212619781494\n",
      "loss over epoch 159 : 4.027186393737793\n",
      "loss over epoch 160 : 4.02716064453125\n",
      "loss over epoch 161 : 4.027135372161865\n",
      "loss over epoch 162 : 4.027109146118164\n",
      "loss over epoch 163 : 4.0270843505859375\n",
      "loss over epoch 164 : 4.027059078216553\n",
      "loss over epoch 165 : 4.027033805847168\n",
      "loss over epoch 166 : 4.027009010314941\n",
      "loss over epoch 167 : 4.026984214782715\n",
      "loss over epoch 168 : 4.026959419250488\n",
      "loss over epoch 169 : 4.026934623718262\n",
      "loss over epoch 170 : 4.026910781860352\n",
      "loss over epoch 171 : 4.026886463165283\n",
      "loss over epoch 172 : 4.026862144470215\n",
      "loss over epoch 173 : 4.026838302612305\n",
      "loss over epoch 174 : 4.0268144607543945\n",
      "loss over epoch 175 : 4.026790618896484\n",
      "loss over epoch 176 : 4.026766777038574\n",
      "loss over epoch 177 : 4.0267438888549805\n",
      "loss over epoch 178 : 4.026721000671387\n",
      "loss over epoch 179 : 4.026697158813477\n",
      "loss over epoch 180 : 4.026674270629883\n",
      "loss over epoch 181 : 4.026651859283447\n",
      "loss over epoch 182 : 4.026628494262695\n",
      "loss over epoch 183 : 4.02660608291626\n",
      "loss over epoch 184 : 4.026583671569824\n",
      "loss over epoch 185 : 4.026561260223389\n",
      "loss over epoch 186 : 4.026538848876953\n",
      "loss over epoch 187 : 4.026516914367676\n",
      "loss over epoch 188 : 4.02649450302124\n",
      "loss over epoch 189 : 4.026473045349121\n",
      "loss over epoch 190 : 4.026451587677002\n",
      "loss over epoch 191 : 4.026430130004883\n",
      "loss over epoch 192 : 4.026408672332764\n",
      "loss over epoch 193 : 4.026386737823486\n",
      "loss over epoch 194 : 4.026366233825684\n",
      "loss over epoch 195 : 4.0263447761535645\n",
      "loss over epoch 196 : 4.026324272155762\n",
      "loss over epoch 197 : 4.026303291320801\n",
      "loss over epoch 198 : 4.02628231048584\n",
      "loss over epoch 199 : 4.026262283325195\n"
     ]
    }
   ],
   "source": [
    "import torch , math\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_dim = 512\n",
    "output_dim = 64\n",
    "lr = 0.1\n",
    "\n",
    "x = torch.randn(1 , 8 , 8 , 512) # (1 , 8 , 8 , 512)\n",
    "\n",
    "\n",
    "weights = torch.randn(input_dim , output_dim , requires_grad = False).unsqueeze(0) #(1 , 512 , 64)\n",
    "bias = torch.zeros(output_dim)\n",
    "\n",
    "alpha = torch.randn(1 , 1 , 1 , 64)\n",
    "b = torch.zeros(64)\n",
    "\n",
    "\n",
    "y_true = torch.randint(0, 64, (1, 8))\n",
    "\n",
    "for epoch in range(1 , 200):\n",
    "    z = torch.matmul(x , weights) + bias #(1 , 8 , 8 , 64)\n",
    "    \n",
    "    mean = z.mean(dim = -1 , keepdim = True) #[1 , 8 , 8 , 1]\n",
    "    std = z.var(dim = -1 , keepdim = True , unbiased = False) #[1 , 8 , 8 , 1]\n",
    "    \n",
    "    xCAP = (z - mean) / (torch.sqrt(std + 1e-9)) \n",
    "    y = xCAP * alpha + b\n",
    "\n",
    "    maxx = torch.max(y , dim = -1 , keepdim = True)[0]\n",
    "    exp = torch.exp(y - maxx)\n",
    "    summ = torch.sum(exp , dim = -1 , keepdim = True)\n",
    "\n",
    "    S = (exp / summ) / z.size(0)\n",
    "    S_mean = S.mean(dim=1)  \n",
    "\n",
    "    log_probs = torch.nn.functional.log_softmax(S_mean, dim=-1)\n",
    "    loss = -log_probs[... , y_true].mean()\n",
    "\n",
    "    ds = torch.nn.functional.softmax(S_mean , dim = 1) # (1 , 8 , 64)\n",
    "    ds[... , y_true] -= 1\n",
    "    ds = ds / (1 * 8)\n",
    "    \n",
    "    dy = ds.unsqueeze(1).expand(-1, 8, -1, -1) / 8  # [1, 8, 8, 64]\n",
    "\n",
    "    n = z.shape[-1]\n",
    "    # dx , dvar , dmean  grads wth direct , variance , mean\n",
    "\n",
    "    d_alpha = (dy * xCAP).sum(dim=(0,1,2), keepdim=True)  # [1, 1, 1, 64]\n",
    "    d_b = dy.sum(dim=(0,1,2), keepdim=True)\n",
    "\n",
    "    dxCAP = dy * alpha\n",
    "\n",
    "    dz_direct = dxCAP / std\n",
    "\n",
    "    #via variance \n",
    "    dz_sum = (dxCAP * xCAP).sum(dim = -1 , keepdim = True)\n",
    "    dz_var = -(1.0 / std) * (xCAP) * (dz_sum / n)\n",
    "\n",
    "    # via mean\n",
    "    dz_sum_ = dxCAP.sum(dim = -1 , keepdim = True)\n",
    "    dz_mean = -(1.0 / std) * (dz_sum_ / n)  \n",
    "\n",
    "    dz = dz_direct + dz_var + dz_mean\n",
    "\n",
    "    d_weights = torch.matmul(x.transpose(-2, -1), dz).sum(dim=1)  # [512, 64]\n",
    "    d_bias = dz.sum(dim=(0,1,2))\n",
    "\n",
    "    d_b_squeezed = d_b.squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weights -= lr * d_weights\n",
    "        bias -= lr * d_bias\n",
    "        alpha -= lr * d_alpha\n",
    "        b -= lr * d_b_squeezed\n",
    "    \n",
    "    print(f\"loss over epoch {epoch} : {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e96765d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9e3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42aebfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43765bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
